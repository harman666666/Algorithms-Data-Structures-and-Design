-> Compressed Tries

-> Binary/Binomial/Fibonacci heaps
-> Splay trees

-> Range Tree
-> KD trees, quad trees, 
-> 2-3 trees, m-n trees

-> Balanced BSTs -> AVL/REDBLACK, Find a python one. 
-> Skip lists
-> Treaps?
-> Suffix Trees, and Suffix tries (http://www.cs.jhu.edu/~langmea/resources/lecture_notes/tries_and_suffix_tries.pdf)

van Emde Boas Trees
B-Trees
Red black trees, 
2-3 search trees
2-3-4 Trees (aka 2-4 trees)
Huffman trees and compression?
Block Cut Trees
Fenwick Tree


LOOOK AT CS240 DATASTRUCTURES NOTES IN FOLDER AND WRITE NOTES HERE FOR 
HASHING
AVL Trees
B TREES
TRIES, COMPRESSED TRIES (PATRICIA TRIES)


##################################################################################
Trie Implementation

This data structure is pretty useful for storing large databases of words. 
It provides linear time search and linear time insertion into the pool of words.

class TrieNode:
    def __init__(self):
        self.children = {}
        self.leaf = False
        
class Trie(object):
 
    def __init__(self):
        """
        Initialize your data structure here.
        """
        self.root = TrieNode()
         
 
    def insert(self, word):
        """
        Inserts a word into the trie.
        :type word: str
        :rtype: void
        """
        root = self.root
        for c in word:
            if c not in root.children:
                root.children[c] = TrieNode()
            root = root.children[c]
        root.leaf = True

    def search(self, word):
        """
        Returns if the word is in the trie.
        :type word: str
        :rtype: bool
        """
        root = self.root
        for c in word:
            if c not in root.children:
                return False
            root = root.children[c]
        return root.leaf
         
 
    def startsWith(self, prefix):
        """
        Returns if there is any word in the trie that starts with the given prefix.
        :type prefix: str
        :rtype: bool
        """
        root = self.root
        for c in prefix:
            if c not in root.children:
                return False
            root = root.children[c]
        return True

##############################################################################

TODO IMPLEMENT IN PYTHON
COMPRESSED TRIES (PATRICIA TRIES)

Compressed Tries (Patricia Tries)
Patricia: Practical Algorithm To Retrieve Information Coded in
Alphanumeric
Introduced by Morrison (1968)
Reduces storage requirement: eliminate unflagged nodes with only
one child
Every path of one-child unflagged nodes is compressed to a single
edge
Each node stores an index indicating the next bit to be tested during
a search (index= 0 for the first bit, index= 1 for the second bit, etc)
A compressed trie storing n keys always has at most n − 1 internal
(non-leaf) node

Each node stores an index indicating the next bit to be tested during
a search (Look at CS240 note for visualization) AND a key, which is the completed word for that node 
from the dictionary of words the trie is generated from 


Search(x):
    Follow the proper path from the root down in the tree to a leaf
    If search ends in an unflagged node, it is unsuccessful
    If search ends in a flagged node, we need to check if the key stored is
    indeed x (because we are skipping checks, and checking random indexes, so we store 
    in each node of the trie, the complete word for that leaf.)

Delete(x):
    Perform Search(x)
    if search ends in an internal node, then
        if the node has two children, then unflag the node and delete the key
        else delete the node and make his only child, the child of its parent
    if search ends in a leaf, then delete the leaf and
    if its parent is unflagged, then delete the parent

Insert(x):
    Perform Search(x)
    -> If the search ends at a leaf L with key y , compare x against y .
    ->If y is a prefix of x, add a child to y containing x.
    ->Else, determine the first index i where they disagree and create a new
        node N with index i.
        Insert N along the path from the root to L so that the parent of N has
        index < i and one child of N is either L or an existing node on the path
        from the root to L that has index > i.
        The other child of N will be a new leaf node containing x.
    ->If the search ends at an internal node, we find the key corresponding to
    that internal node and proceed in a similar way to the previous case.


##################################################################################
##################################################################################
##################################################################################
STRING SEARCH ALGORITHMS:
        KMP string searching
        Manacher’s Algorithm
        Aho–Corasick string matching algorithm;
        Z ALGORITHM

###########################################################################
#############################################################################

Here are some typical thoughts on search types:

Boyer-Moore: works by pre-analyzing the pattern and comparing from right-to-left. 
If a mismatch occurs, the initial analysis is used to determine how far the pattern can 
be shifted w.r.t. the text being searched. This works particularly well for long search 
patterns. In particular, it can be sub-linear, as you do not need to read 
every single character of your text.

Knuth-Morris-Pratt: also pre-analyzes the pattern, but tries to re-use whatever 
was already matched in the initial part of the pattern to avoid having to rematch that. 
This can work quite well, if your alphabet is small (f.ex. DNA bases), as you get a 
higher chance that your search patterns contain reuseable subpatterns.

Aho-Corasick: Needs a lot of preprocessing, but does so for a number of patterns. 
If you know you will be looking for the same search patterns over and over again, 
then this is much better than the other, because you need to 
analyse patterns only once, not once per search.

Space Usage favors Rabin-Karp
One major advantage of Rabin-Karp is that it uses O(1) auxiliary storage space, 
which is great if the pattern string you're looking for is very large. 
For example, if you're looking for all occurrences of a string of length 107 
in a longer string of length 109, not having to allocate a table of 107 
machine words for a failure function or shift table is a major win. 
Both Boyer-Moore and KMP use Ω(n) memory on a pattern string of 
length n, so Rabin-Karp would be a clear win here.

Worst-Case Single-Match Efficiency Favors Boyer-Moore or KMP
Rabin-Karp suffers from two potential worst cases. First, 
if the particular prime numbers used by Rabin-Karp are 
known to a malicious adversary, that adversary could
potentially craft an input that causes the rolling hash to match the 
hash of a pattern string at each point in time, causing the algorithm's 
performance to degrade to Ω((m - n + 1)n) on a string of length m and 
pattern of length n. If you're taking untrusted strings as input, this 
could potentially be an issue. Neither Boyer-Moore 
nor KMP have these weaknesses.

Worst-Case Multiple-Match Efficiency favors KMP.
Similarly, Rabin-Karp is slow in the case where you want to find all matches of a 
pattern string in the case where that pattern appears a large number of times. 
For example, if you're looking for a string of 105 copies of the letter a 
in text string consisting of 109copies of the letter a with Rabin-Karp, 
then there will be lots of spots where the pattern string appears, 
and each will require a linear scan. This can also lead 
to a runtime of Ω((m + n - 1)n).

Many Boyer-Moore implementations suffer from this second rule, but will not 
have bad runtimes in the first case. 
And KMP has no pathological worst-cases like these.

Best-Case Performance favors Boyer-Moore
One advantage of the Boyer-Moore algorithm is that it doesn't necessarily 
have to scan all the characters of the input string. Specifically, 
the Bad Character Rule can be used to skip over huge regions of 
the input string in the event of a mismatch. More specifically, 
the best-case runtime for Boyer-Moore is O(m / n), which is much 
faster than what Rabin-Karp or KMP can provide.

Generalizations to Multiple Strings favor KMP
Suppose you have a fixed set of multiple text strings that you want to 
search for, rather than just one. You could, if you wanted to, run multiple passes 
of Rabin-Karp, KMP, or Boyer-Moore across the strings to find all the matches. 
However, the runtime of this approach isn't great, as it scales linearly 
with the number of strings to search for. On the other hand, KMP generalizes 
nicely to the Aho-Corasick string-matching algorithm, which runs in time
O(m + n + z), where z is the number of matches found and n is the 
combined length of the pattern strings. Notice that there's no dependence 
here on the number of different pattern strings being searched for!

#################################################################################
STRING ALGORITHM: RABIN KARP

    Rabin Karp
        So Rabin Karp algorithm needs to calculate 
        hash values for following strings.
        1) Pattern itself.
        2) All the substrings of text of length m.

        Since we need to efficiently calculate hash values for all the substrings 
        of size m of text, we must have a hash function which has following property.
        Hash at the next shift must be efficiently computable from the current hash 
        value and next character in text or we can say hash(txt[s+1 .. s+m]) must 
        be efficiently computable from hash(txt[s .. s+m-1]) and txt[s+m] i.e., 
        hash(txt[s+1 .. s+m])= rehash(txt[s+m], hash(txt[s .. s+m-1])) and rehash must be O(1) operation.
        
        To do rehashing, we need to take off the most significant digit
        and add the new least significant digit for in hash value. 
        Rehashing is done using the following formula.
        
        prevHash = hash( txt[s .. s+m-1])
        hash( txt[s+1 .. s+m] ) = ( d * ( prevHash – txt[s]*h ) + txt[s + m] ) mod q

        hash( txt[s .. s+m-1] ) : Hash value at shift s.
        hash( txt[s+1 .. s+m] ) : Hash value at next shift (or shift s+1)
        d: Number of characters in the alphabet
        q: A prime number
        h: d^(m-1)

        How does above expression work?
        This is simple mathematics, we compute decimal value of current window from previous window.
        For example pattern length is 3 and string is “23456”
        You compute the value of first window (which is “234”) as 234.
        How how will you compute value of next window “345”? You will do (234 – 2*100)*10 + 5 and get 345.

        # Rabin Karp Algorithm given in CLRS book 
        # d is the number of characters in the input alphabet 
        d = 256

        # pat  -> pattern 
        # txt  -> text 
        # q    -> A prime number 
        
        def search(pat, txt, q): 
            M = len(pat) 
            N = len(txt) 
            i = 0
            j = 0
            p = 0    # hash value for pattern 
            t = 0    # hash value for txt 
            h = 1
        
            # The value of h would be "pow(d, M-1)%q" 
            for i in xrange(M-1): 
                h = (h*d)%q 
        
            # Calculate the hash value of pattern and first window 
            # of text 
            for i in xrange(M): 
                p = (d*p + ord(pat[i]))%q 
                t = (d*t + ord(txt[i]))%q 
        
            # Slide the pattern over text one by one 
            for i in xrange(N-M+1): 
                # Check the hash values of current window of text and 
                # pattern if the hash values match then only check 
                # for characters on by one 
                if p==t: 
                    # Check for characters one by one 
                    for j in xrange(M): 
                        if txt[i+j] != pat[j]: 
                            break
        
                    j+=1
                    # if p == t and pat[0...M-1] = txt[i, i+1, ...i+M-1] 
                    if j==M: 
                        print "Pattern found at index " + str(i) 
        
                # Calculate hash value for next window of text: Remove 
                # leading digit, add trailing digit 
                if i < N-M: 
                    t = (d*(t-ord(txt[i])*h) + ord(txt[i+M]))%q 
        
                    # We might get negative values of t, converting it to 
                    # positive 
                    if t < 0: 
                        t = t+q 
        
        # Driver program to test the above function 
        txt = "GEEKS FOR GEEKS"
        pat = "GEEK"
        q = 101 # A prime number 
        search(pat,txt,q) 
        
        # This code is contributed by Bhavya Jain 


#################################################################################
STRING ALGORITHM: FINITE AUTOMATA (Basically KMP)

The basic idea is to build a automaton in which
• Each character in the pattern has a state.
• Each match sends the automaton into a new state.
• If all the characters in the pattern has been
matched, the automaton enters the accepting state.
• Otherwise, the automaton will return to a suitable
state according to the current state and the input
character such that this returned state reflects the
maximum advantage we can take from the
previous matching.
• the matching takes O(n) time since each character
is examined once.



The construction of the stringmatching automaton is based on
the given pattern. The time of this
construction may be O(m3
|S|)

A finite automaton M is a 5-tuple (Q,q0
,A,S,d),
where
• Q is a finite set of states.
• q0 in Q is the start state.
• A in Q is a distinguish set of accepting states.
• S is a finite input alphabet
• d is a function from Q × S into Q, called the
transition function of M.

Construction of the FA is the main tricky part of this algorithm. 
Once the FA is built, the searching is simple. In search, we simply 
need to start from the first state of the automata and the first 
character of the text. At every step, we consider next character of 
text, look for the next state in the built FA and move to a new state. 
If we reach the final state, then the pattern is found in the text. 
The time complexity of the search process is O(n).

Matching time on a text string of length n is Θ(n)
This does not include the preprocessing time required to compute the
transition function δ. There exists an algorithm with O(m|Σ|)
preprocessing time.
Altogether, we can find all occurrences of a length-m pattern in a
length-n text over a finite alphabet Σ with O(m|Σ|) preprocessing
time and Θ(n) matching time.

    # Python program for Finite Automata  
    # Pattern searching Algorithm 
    
    NO_OF_CHARS = 256
    
    def getNextState(pat, M, state, x): 
        ''' 
        calculate the next state  
        '''
        # If the character c is same as next character  
        # in pattern, then simply increment state 

        if state < M and x == ord(pat[state]): 
            return state+1

        i=0
        # ns stores the result which is next state 
        # ns finally contains the longest prefix  
        # which is also suffix in "pat[0..state-1]c" 
    
        # Start from the largest possible value and  
        # stop when you find a prefix which is also suffix 
        for ns in range(state,0,-1): 
            if ord(pat[ns-1]) == x: 
                while(i<ns-1): 
                    if pat[i] != pat[state-ns+1+i]: 
                        break
                    i+=1
                if i == ns-1: 
                    return ns  
        return 0
    
    def computeTF(pat, M): 
        ''' 
        This function builds the TF table which  
        represents Finite Automata for a given pattern 
        '''
        global NO_OF_CHARS 
    
        TF = [[0 for i in range(NO_OF_CHARS)]\ 
            for _ in range(M+1)] 
    
        for state in range(M+1): 
            for x in range(NO_OF_CHARS): 
                z = getNextState(pat, M, state, x) 
                TF[state][x] = z 
    
        return TF 
    
    def search(pat, txt): 
        ''' 
        Prints all occurrences of pat in txt 
        '''
        global NO_OF_CHARS 
        M = len(pat) 
        N = len(txt) 
        TF = computeTF(pat, M)     
    
        # Process txt over FA. 
        state=0
        for i in range(N): 
            state = TF[state][ord(txt[i])] 
            if state == M: 
                print("Pattern found at index: {}".\ 
                    format(i-M+1)) 
    
    # Driver program to test above function             
    def main(): 
        txt = "AABAACAADAABAAABAA"
        pat = "AABA"
        search(pat, txt) 
    
    if __name__ == '__main__': 
        main() 

#################################################################################
STRING ALGORITHM: KMP

Compares the pattern to the text in left-to-right
Shifts the pattern more intelligently than the brute-force algorithm
When a mismatch occurs, how much can we shift the pattern
(reusing knowledge from previous matches)?

KMP Answer: this depends on the largest prefix of P[0..j] that is a
suffix of P[1..j]

Prefix of P[0..j]: starts from index 0 of P and builds to right 
Suffix of P[1..j]: starts from P[j] and grows by building left, and you cant include P[0] (because its P[1..j])


    isSubstring(), KMP Algorithm
        
        A linear time (!) algorithm that solves the string matching
        problem by preprocessing P in Θ(m) time
        – Main idea is to skip some comparisons by using the previous
        comparison result
        
        LPS[] that will hold the longest prefix suffix  
        
        Uses an auxiliary array π that is defined as the following:
        – π[i] is the largest integer smaller than i such that P 1 . . . P π[i] is
        a suffix of P 1 . . . P i

        Examples:

        Pattern: a a a a a
        LSP    : 0 1 2 3 4

        Pattern: a b a b a b
        LSP    : 0 0 1 2 3 4

        Pattern: a b a c a b a b
        LSP    : 0 0 1 0 1 2 3 2

        Pattern: a a a b a a a a a b
        LSP    : 0 1 2 0 1 2 3 3 3 4

        txt[] = "AAAAABAAABA" 
        pat[] = "AAAA"
        lps[] = {0, 1, 2, 3} 

        i = 0, j = 0
        txt[] = "AAAAABAAABA" 
        pat[] = "AAAA"
        txt[i] and pat[j] match, do i++, j++

        i = 1, j = 1
        txt[] = "AAAAABAAABA" 
        pat[] = "AAAA"
        txt[i] and pat[j] match, do i++, j++

        i = 2, j = 2
        txt[] = "AAAAABAAABA" 
        pat[] = "AAAA"
        pat[i] and pat[j] match, do i++, j++

        i = 3, j = 3
        txt[] = "AAAAABAAABA" 
        pat[] = "AAAA"
        txt[i] and pat[j] match, do i++, j++

        i = 4, j = 4
        Since j == M, print pattern found and reset j,
        j = lps[j-1] = lps[3] = 3

        Here unlike Naive algorithm, we do not match first three 
        characters of this window. Value of lps[j-1] (in above 
        step) gave us index of next character to match.
        i = 4, j = 3
        txt[] = "AAAAABAAABA" 
        pat[] =  "AAAA"
        txt[i] and pat[j] match, do i++, j++

        i = 5, j = 4
        Since j == M, print pattern found and reset j,
        j = lps[j-1] = lps[3] = 3

        Again unlike Naive algorithm, we do not match first three 
        characters of this window. Value of lps[j-1] (in above 
        step) gave us index of next character to match.
        i = 5, j = 3
        txt[] = "AAAAABAAABA" 
        pat[] =   "AAAA"
        txt[i] and pat[j] do NOT match and j > 0, change only j
        j = lps[j-1] = lps[2] = 2

        i = 5, j = 2
        txt[] = "AAAAABAAABA" 
        pat[] =    "AAAA"
        txt[i] and pat[j] do NOT match and j > 0, change only j
        j = lps[j-1] = lps[1] = 1 

        i = 5, j = 1
        txt[] = "AAAAABAAABA" 
        pat[] =     "AAAA"
        txt[i] and pat[j] do NOT match and j > 0, change only j
        j = lps[j-1] = lps[0] = 0

        i = 5, j = 0
        txt[] = "AAAAABAAABA" 
        pat[] =      "AAAA"
        txt[i] and pat[j] do NOT match and j is 0, we do i++.

        i = 6, j = 0
        txt[] = "AAAAABAAABA" 
        pat[] =       "AAAA"
        txt[i] and pat[j] match, do i++ and j++

        i = 7, j = 1
        txt[] = "AAAAABAAABA" 
        pat[] =       "AAAA"
        txt[i] and pat[j] match, do i++ and j++

        We continue this way...

        def KMPSearch(pat, txt): 
            M = len(pat) 
            N = len(txt) 
        
            # create lps[] that will hold the longest prefix suffix  
            # values for pattern 
            lps = [0]*M 
            j = 0 # index for pat[] 
        
            # Preprocess the pattern (calculate lps[] array) 
            computeLPSArray(pat, M, lps) 
        
            i = 0 # index for txt[] 
            while i < N: 
                if pat[j] == txt[i]: 
                    i += 1
                    j += 1
        
                if j == M: 
                    print "Found pattern at index " + str(i-j) 
                    j = lps[j-1] 
        
                # mismatch after j matches 
                elif i < N and pat[j] != txt[i]: 
                    # Do not match lps[0..lps[j-1]] characters, 
                    # they will match anyway 
                    if j != 0: 
                        j = lps[j-1] 
                    else: 
                        # couldnt match any j, skip this i, its bad. 
                        i += 1
        
        def computeLPSArray(pat, M, lps): 
            len = 0 # length of the previous longest prefix suffix 
        
            lps[0] # lps[0] is always 0 
            i = 1
        
            # the loop calculates lps[i] for i = 1 to M-1 
            while i < M: 
                if pat[i]== pat[len]: 
                    len += 1
                    lps[i] = len
                    i += 1
                else: 
                    # This is tricky. Consider the example. 
                    # AAACAAAA and i = 7. The idea is similar  
                    # to search step. 
                    if len != 0: 
                        len = lps[len-1] 
        
                        # Also, note that we do not increment i here 
                    else: 
                        lps[i] = 0
                        i += 1
        
        txt = "ABABDABACDABABCABAB"
        pat = "ABABCABAB"
        KMPSearch(pat, txt) 
#################################################################################
STRING ALGORITHM: BOYER MOORE WALKTHROUGH:

PART A:
    Based on three key ideas:
        Reverse-order searching: Compare P with a subsequence of T moving
        backwards

        Bad character jumps: When a mismatch occurs at T [i] = c
            If P contains c, we can shift P to align the last occurrence of c in P
            with T [i]
            Otherwise, we can shift P to align P[0] with T [i + 1]
        
        Good suffix jumps: If we have already matched a suffix of P, then get
            a mismatch, we can shift P forward to align with the previous
            occurence of that suffix (with a mismatch from the suffix we read). If
            none exists, look for the longest prefix of P that is a suffix of what we
            read. Similar to failure array in KMP.

        Can skip large parts of T

    Boyer Moore is a combination of following two approaches.
    1) Bad Character Heuristic
    2) Good Suffix Heuristic

    Both of the above heuristics can also be used independently to search a 
    pattern in a text. Let us first understand how two independent 
    approaches work together in the Boyer Moore algorithm. If we 
    take a look at the Naive algorithm, it slides the pattern over 
    the text one by one. KMP algorithm does preprocessing over the 
    pattern so that the pattern can be shifted by more than one. 
    The Boyer Moore algorithm does preprocessing for the same reason. 
    It processes the pattern and creates different arrays for both 
    heuristics. At every step, it slides the pattern by the max of the 
    slides suggested by the two heuristics. So it uses best 
    of the two heuristics at every step.

    Unlike the previous pattern searching algorithms, Boyer Moore algorithm 
    starts matching from the last character of the pattern.

PART B:
    The insight behind Boyer-Moore is that if you start searching for a 
    pattern in a string starting with the last character in the pattern, 
    you can jump your search forward multiple characters when you hit a mismatch.

    Let's say our pattern p is the sequence of characters 
    p1, p2, ..., pn and we are searching a 
    string s, currently with p aligned so that pn is at index i in s.

    E.g.:

    s = WHICH FINALLY HALTS.  AT THAT POINT...
    p = AT THAT
    i =       ^
    The B-M paper makes the following observations:
    (1) if we try matching a character that is not in p then we can jump forward n characters:

    'F' is not in p, hence we advance n characters:

    s = WHICH FINALLY HALTS.  AT THAT POINT...
    p =        AT THAT
    i =              ^

    (2) if we try matching a character whose last position is 
    k from the end of p then we can jump forward k characters:

    ' 's last position in p is 4 from the end, hence we advance 4 characters:

    s = WHICH FINALLY HALTS.  AT THAT POINT...
    p =            AT THAT
    i =                  ^

    Now we scan backwards from i until we either succeed or we hit a mismatch. 
    (3a) if the mismatch occurs k characters from the start of p and the 
    mismatched character is not in p, then we can advance (at least) k characters.

    'L' is not in p and the mismatch occurred against p6, hence we can advance (at least) 6 characters:

    s = WHICH FINALLY HALTS.  AT THAT POINT...
    p =                  AT THAT
    i =                        ^
    However, we can actually do better than this. (3b) since we know that 
    at the old i we'd already matched some characters (1 in this case). If 
    the matched characters don't match the start of p, then we can actually 
    jump forward a little more (this extra distance is called 'delta2' in the paper):

    s = WHICH FINALLY HALTS.  AT THAT POINT...
    p =                   AT THAT
    i =                         ^
    At this point, observation (2) applies again, giving

    s = WHICH FINALLY HALTS.  AT THAT POINT...
    p =                       AT THAT
    i =                             ^
    and bingo! We're done.

PART C:

    Now that said, the algorithm is based on a simple principle. Suppose that I'm trying 
    to match a substring of length m. I'm going to first look at character at index m.
    If that character is not in my string, I know that the substring 
    I want can't start in characters at indices 1, 2, ... , m.

    If that character is in my string, I'll assume that it is at the last place in my 
    string that it can be. I'll then jump back and start trying to match my string 
    from that possible starting place. This piece of information is my first table.

    Once I start matching from the beginning of the substring, when I find a mismatch, 
    I can't just start from scratch. I could be partially through a match starting 
    at a different point. For instance if I'm trying to match anand in ananand successfully 
    match, anan, realize that the following a is not a d, but I've just matched an, and so 
    I should jump back to trying to match my third character in my substring. This, "If I fail 
    after matching x characters, I could be on the y'th character of a match" 
    information is stored in the second table.

    Note that when I fail to match the second table knows how far along in a match I might 
    be based on what I just matched. The first table knows how far back I might be based 
    on the character that I just saw which I failed to match. You want to use 
    the more pessimistic of those two pieces of information.

    With this in mind the algorithm works like this:

    start at beginning of string
    start at beginning of match
    while not at the end of the string:
        if match_position is 0:
            Jump ahead m characters
            Look at character, jump back based on table 1
            If match the first character:
                advance match position
            advance string position
        else if I match:
            if I reached the end of the match:
            FOUND MATCH - return
            else:
            advance string position and match position.
        else:
            pos1 = table1[ character I failed to match ]
            pos2 = table2[ how far into the match I am ]
            if pos1 < pos2:
                jump back pos1 in string
                set match position at beginning
            else:
                set match position to pos2
    FAILED TO MATCH



#################################################################################
STRING ALGORITHM: BOYER Moore BAD CHARACTER HEURISTIC

Boyer-Moore Algorithm

In this post, we will discuss bad character heuristic, 
and discuss Good Suffix heuristic in the next post.

The idea of bad character heuristic is simple. 
The character of the text which doesn’t match with the current 
character of the pattern is called the Bad Character.
Upon mismatch, we shift the pattern until –
1) The mismatch becomes a match
2) Pattern P move past the mismatched character.

Case 1 – Mismatch become match
We will lookup the position of last occurrence of mismatching character 
in pattern and if mismatching character exist in pattern then we’ll shift 
the pattern such that it get aligned to the mismatching character in text T.

Case 2 – Pattern move past the mismatch character
We’ll lookup the position of last occurrence of mismatching character 
in pattern and if character does not exist we will 
shift pattern past the mismatching character.

NO_OF_CHARS = 256
  
def badCharHeuristic(string, size): 
    ''' 
    The preprocessing function for 
    Boyer Moore's bad character heuristic 
    '''
  
    # Initialize all occurrence as -1 
    badChar = [-1]*NO_OF_CHARS 
  
    # Fill the actual value of last occurrence 
    for i in range(size): 
        badChar[ord(string[i])] = i; 
  
    # retun initialized list 
    return badChar 
  
def search(txt, pat): 
    ''' 
    A pattern searching function that uses Bad Character 
    Heuristic of Boyer Moore Algorithm 
    '''
    m = len(pat) 
    n = len(txt) 
  
    # create the bad character list by calling  
    # the preprocessing function badCharHeuristic() 
    # for given pattern 
    badChar = badCharHeuristic(pat, m)  
  
    # s is shift of the pattern with respect to text 
    s = 0
    while(s <= n-m): 
        j = m-1
  
        # Keep reducing index j of pattern while  
        # characters of pattern and text are matching 
        # at this shift s 
        while j>=0 and pat[j] == txt[s+j]: 
            j -= 1
  
        # If the pattern is present at current shift,  
        # then index j will become -1 after the above loop 
        if j<0: 
            print("Pattern occur at shift = {}".format(s)) 
  
            '''     
                Shift the pattern so that the next character in text 
                      aligns with the last occurrence of it in pattern. 
                The condition s+m < n is necessary for the case when 
                   pattern occurs at the end of text 
               '''
            s += (m-badChar[ord(txt[s+m])] if s+m<n else 1) 
        else: 
            ''' 
               Shift the pattern so that the bad character in text 
               aligns with the last occurrence of it in pattern. The 
               max function is used to make sure that we get a positive 
               shift. We may get a negative shift if the last occurrence 
               of bad character in pattern is on the right side of the 
               current character. 
            '''
            s += max(1, j-badChar[ord(txt[s+j])]) 
  
  
# Driver program to test above function 
def main(): 
    txt = "ABAAABCD"
    pat = "ABC"
    search(txt, pat) 
  
if __name__ == '__main__': 
    main() 
  
#################################################################################
STRING ALGORITHM: BOYER Moore GOOD CHARACTER HEURISTIC

https://www.geeksforgeeks.org/boyer-moore-algorithm-good-suffix-heuristic/

#################################################################################
STRING ALGORITHM: BOYER Moore COMBINED

COMPLETE THIS THANK YOU!!

#################################################################################
STRING ALGORITHM: Suffix Tries and Suffix Trees

What if we want to search for many patterns P within the same fixed
text T ?

Idea: Preprocess the text T rather than the pattern P
Observation: P is a substring of T if and only if P is a prefix of some
suffix of T .
We will call a trie that stores all suffixes of a text T a suffix trie, and the
compressed suffix trie of T a suffix tree.

Build the suffix trie, i.e. the trie containing all the suffixes of the text
Build the suffix tree by compressing the trie above (like in Patricia
trees)
Store two indexes l, r on each node v (both internal nodes and
leaves) where node v corresponds to substring T [l..r ]

T = bananaban
SUFFIXES: {bananaban$, ananaban$, nanaban$, anaban$, naban$, aban$, ban$, an$, n$}

Use $ to indicate end of suffixes in trie
CREATE TRIE!
then create compressed trie aka suffix tree (and each node contains 2 indexes l,r )


Suffix Trees: Pattern Matching
    To search for pattern P of length m:
    Similar to Search in compressed trie with the difference that we are
    looking for a prefix match rather than a complete match
    If we reach a leaf with a corresponding string length less than m, then
    search is unsuccessful
    Otherwise, we reach a node v (leaf or internal) with a corresponding
    string length of at least m
    It only suffices to check the first m characters against the substring of
    the text between indices of the node, to see if there indeed is a match
    We can then visit all children of the node to report all matches


#########################################################################
########################################################################3

Aho-Corasick algorithm (https://cp-algorithms.com/string/aho_corasick.html)

    The Aho-Corasick algorithm constructs a data structure similar 
    to a trie with some additional links, and then constructs a finite state 
    machine (automaton) in O(mk) time, where k is the size of the used alphabet.


    Formally a trie is a rooted tree, where each edge of the tree is labeled by 
    some letter. All outgoing edge from one vertex mush have different labels.

    Consider any path in the trie from the root to any vertex. If we write out 
    the labels of all edges on the path, we get a string that corresponds to 
    this path. For any vertex in the trie we will associate the string 
    from the root to the vertex.

    Each vertex will also have a flag leaf which will be true, if any string 
    from the given set corresponds to this vertex.

    Accordingly to build a trie for a set of strings means to build a trie such 
    that each leaf vertex will correspond to one string from the set, and 
    conversely that each string of the set corresponds to one leaf vertex.

    We introduce a structure for the vertices of the tree.
    _______________________
    const int K = 26;

    struct Vertex {
        int next[K];
        bool leaf = false;

        Vertex() {
            fill(begin(next), end(next), -1);
        }
    };

    vector<Vertex> trie(1);
    _______________________
    To add strings to trie:
    Now we implement a function that will add a string s to the trie. 
    The implementation is extremely simple: we start at the root node, 
    and as long as there are edges corresponding to the characters of
    s we follow them. If there is no edge for one character, we simply 
    generate a new vertex and connect it via an edge. At the end of the 
    process we mark the last vertex with flag leaf.
    _______________________
    void add_string(string const& s) {
        int v = 0;
        for (char ch : s) {
            int c = ch - 'a';
            if (trie[v].next[c] == -1) {
                trie[v].next[c] = trie.size();
                trie.emplace_back(); // Default construction of the vector!
            }
            v = trie[v].next[c];
        }
        trie[v].leaf = true;
    }   
    _______________________
    The implementation obviously runs in linear time. And 
    since every vertex store k links, it will use O(mk) memory.

    It is possible to decrease the memory consumption to O(m) by using a 
    map instead of an array in each vertex. 
    However this will increase the complexity to O(nlogk).

    Construction of an automaton
    Suppose we have built a trie for the given set of strings. Now let's 
    look at it from a different side. If we look at any vertex. The string that 
    corresponds to it is a prefix of one or more strings in the set, thus 
    each vertex of the trie can be interpreted as a position in 
    one or more strings from the set.

    In fact the trie vertices can be interpreted as states in a finite deterministic 
    automaton. From any state we can transition - using some input letter - to 
    other states, i.e. to another position in the set of strings. For example, 
    if there is only one string in the trie abc, and we are standing at 
    vertex 2 (which corresponds to the string ab), then using the 
    letter c we can transition to the state 3.

    Thus we can understand the edges of the trie as transitions in an 
    automaton according to the corresponding letter. 
    However for an automaton we cannot restrict the possible transitions 
    for each state. If we try to perform a transition using a letter, 
    and there is no corresponding edge in the trie, then we 
    nevertheless must go into some state.

    More strictly, let us be in a state p corresponding to the string t, 
    and we want to transition to a different state with the character c.
    If there is an edge labeled with this letter c, then we can simply go 
    over this edge, and get the vertex corresponding to t+c. If there is no such edge, 
    then we must find the state corresponding to the longest proper suffix 
    of the string t (the longest available in the trie), and try to perform 
    a transition via c from there.

    For example let the trie be constructed by the strings ab and bc, 
    and we are currently at the vertex corresponding to ab, which 
    is a leaf. For a transition with the letter c, we are forced to go
    to the state corresponding to the string b, and from 
    there follow the edge with the letter c.

    A suffix link for a vertex p is a edge that points to the longest 
    proper suffix of the string corresponding to the vertex p. 
    The only special case is the root of the trie, the suffix 
    link will point to itself. Now we can reformulate the statement 
    about the transitions in the automaton like this: while from the 
    current vertex of the trie there is no transition using the current 
    letter (or until we reach the root), we follow the suffix link.

    Thus we reduced the problem of constructing an automaton to 
    the problem of finding suffix links for all vertices of the trie. 
    However we will build these suffix links, oddly enough, 
    using the transitions constructed in the automaton.

    Note that if we want to find a suffix link for some vertex v, 
    then we can go to the ancestor p of the current vertex 
    (let c be the letter of the edge from p to v), then follow its 
    suffix link, and perform from there the transition with the letter c.

    Thus the problem of finding the transitions has been reduced 
    to the problem of finding suffix links, and the problem of 
    finding suffix links has been reduced to the problem of finding 
    a suffix link and a transition, but for vertices closer to the root. 
    So we have a recursive dependence that we can resolve in linear time.

    Let's move to the implementation. Note that we now will store 
    the ancestor p and the character pch of the edge from p to v for 
    each vertex v. Also at each vertex we will store the suffix 
    link link (or −1 if it hasn't been calculated yet), and in the 
    array go[k] the transitions in the machine for each symbol 
    (again −1 if it hasn't been calculated yet).
    _________________________________________________
    const int K = 26;

    struct Vertex {
        int next[K];
        bool leaf = false;
        int p = -1;
        char pch;
        int link = -1;
        int go[K];

        Vertex(int p=-1, char ch='$') : p(p), pch(ch) {
            fill(begin(next), end(next), -1);
            fill(begin(go), end(go), -1);
        }
    };

    vector<Vertex> t(1);

    void add_string(string const& s) {
        int v = 0;
        for (char ch : s) {
            int c = ch - 'a';
            if (t[v].next[c] == -1) {
                t[v].next[c] = t.size();
                // emplace_back is getting default args, parent and char.
                t.emplace_back(v, ch); 
            }
            v = t[v].next[c];
        }
        t[v].leaf = true;
    }

    int go(int v, char ch);

    int get_link(int v) {
        if (t[v].link == -1) {
            if (v == 0 || t[v].p == 0)
                t[v].link = 0;
            else
                t[v].link = go(get_link(t[v].p), t[v].pch);
        }
        return t[v].link;
    }

    int go(int v, char ch) {
        int c = ch - 'a';
        if (t[v].go[c] == -1) {
            if (t[v].next[c] != -1)
                t[v].go[c] = t[v].next[c];
            else
                t[v].go[c] = v == 0 ? 0 : go(get_link(v), ch);
        }
        return t[v].go[c];
    } 
    _________________________________________________
    It is easy to see, that due to the memoization of the found suffix 
    links and transitions the total time for finding all suffix 
    links and transitions will be linear.
################################################################################
###############################################################################

    Aho-Corasick Python Implementation: 
    (https://carshen.github.io/data-structures/algorithms/2014/04/07/aho-corasick-implementation-in-python.html)


    Implementation of the Aho-Corasick algorithm in Python
    07 Apr 2014

    Recall from last time that we needed to construct the trie, and then 
    set its failure transitions. After the trie is constructed, we traverse the 
    trie as we are reading in the input text and output the positions that 
    we find the keywords at. Essentially, these three parts form the structure of the algorithm.

    The trie is represented as an adjacency list. One row of the adjacency 
    list represents one node, and the index of the row is the unique id of that node. 
    The row contains a dict {'value':'', 'next_states':[],'fail_state':0,'output':[]}
    where value is the character the node represents ('a','b', '#', '$', etc), 
    next_states is a list of the id's of the child nodes, 
    fail_state is the id of the fail state, and output is a list of 
    all the complete keywords we have encountered so far as we have gone 
    through the input text (in this implementation, we can add the same word multiple times in the trie).

    We initialize the trie, called AdjList, and add the root 
    node. We have the keywords, which we will add one by one into the trie.

    1 from collections import deque
    2 AdjList = []
    3 
    4 def init_trie(keywords):
    5  """ creates a trie of keywords, then sets fail transitions """
    6  create_empty_trie()
    7  add_keywords(keywords)
    8  set_fail_transitions()
    9 
    10 def create_empty_trie():
    11  """ initalize the root of the trie """
    12  AdjList.append({'value':'', 'next_states':[],'fail_state':0,'output':[]})
    13 
    14 def add_keywords(keywords):
    15  """ add all keywords in list of keywords """
    16  for keyword in keywords:
    17      add_keyword(keyword)

    We also write a helper find_next_state which takes a node and a 
    value, and returns the id of the child of that node whose 
    value matches value, or else None if none found.

    1 def find_next_state(current_state, value):
    2   for node in AdjList[current_state]["next_states"]:
    3       if AdjList[node]["value"] == value:
    4           return node
    5   return None

    Note that this trie only handles lowercase words, for simplicity for my testing. 
    To add a keyword into the trie, we traverse the longest prefix of 
    the keyword that exists in the trie starting from the root, 
    then we add the characters of the rest of the keyword as nodes in the trie, in a chain.

    1 def add_keyword(keyword):
    2  """ add a keyword to the trie and mark output at the last node """
    3  current_state = 0
    4  j = 0
    5  keyword = keyword.lower()
    6  child = find_next_state(current_state, keyword[j])
    7  while child != None:
    8      current_state = child
    9      j = j + 1
    10      if j < len(keyword):
    11          child = find_next_state(current_state, keyword[j])
    12      else:
    13          break
    14  for i in range(j, len(keyword)):
    15      node = {'value':keyword[i],'next_states':[],'fail_state':0,'output':[]}
    16      AdjList.append(node)
    17      AdjList[current_state]["next_states"].append(len(AdjList) - 1)
    18      current_state = len(AdjList) - 1
    19  AdjList[current_state]["output"].append(keyword)

    The while loop finds the longest prefix of the keyword which exists in the 
    trie so far, and will exit when we can no longer match more characters 
    at index j. The for loop goes through the rest of the keyword, creating 
    a new node for each character and appending it to AdjList. len(AdjList) - 1 
    gives the id of the node we are appending, since we are adding to the end of AdjList.

    When we have completed adding the keyword in the trie, 
    AdjList[current_state]["output"].append(keyword) will append the 
    keyword to the output of the last node, to mark the end of the keyword at that node.

    Now, to set the fail transitions. We will do a breadth first search 
    over the trie and set the failure state of each node. First, we set 
    all the children of the root to have the failure state of the root, 
    since the longest strict suffix of a character would be the empty string, 
    represented by the root. The failure state of the root doesn't matter, 
    since when we get to the root, we will just leave the loop, 
    but we can just set it to be the root itself.

    Remember that the failure state indicates the end of 
    the next longest proper suffix of the string that we have currently matched.

    Consider the node r. We are setting the failure state for node child of r. 
    Initially the potential parent of the fail state of child, 
    state will be the next longest proper suffix, which is marked by r's fail state. 
    If there is no transition from r's fail state to a node with the same 
    value as child, then we go to the next longest proper suffix, 
    which is the fail state of r's fail state, and so on, until we 
    find one which works, or we are at the root. 
    We set child's fail state to be this fail state.

    We append the output of the fail state to child's output 
    because since the fail state is a suffix of the string which ends 
    at child, whatever matched words found at the fail state will also occur 
    at child. If we did not keep this line, we would miss 
    out on substrings of the currently matched string which are keywords.

    1 def set_fail_transitions():
    2  q = deque()
    3  child = 0
    4  for node in AdjList[0]["next_states"]:
    5      q.append(node)
    6      AdjList[node]["fail_state"] = 0
    7  while q:
    8      r = q.popleft()
    9      for child in AdjList[r]["next_states"]:
    10          q.append(child)
    11          state = AdjList[r]["fail_state"] # parents fail state
    12          while find_next_state(state, AdjList[child]["value"]) == None and state != 0:
    14              state = AdjList[state]["fail_state"]
    15          AdjList[child]["fail_state"] = find_next_state(state, AdjList[child]["value"])
    17          if AdjList[child]["fail_state"] is None:
    18              AdjList[child]["fail_state"] = 0
    19          AdjList[child]["output"] = AdjList[child]["output"] + AdjList[AdjList[child]["fail_state"]]["output"]

    Finally, our trie is constructed. Given an input, line, we iterate through each 
    character in line, going up to the fail state when we no longer match the next 
    character in line. At each node, we check to see if there is any output, and 
    we will capture all the outputted words and their respective indices. 
    (i-len(j) + 1 is for writing an index at the beginning of the word)

    1 def get_keywords_found(line):
    2  """ returns true if line contains any keywords in trie """
    3  line = line.lower()
    4  current_state = 0
    5  keywords_found = []
    6 
    7  for i in range(len(line)):
    8      while find_next_state(current_state, line[i]) is None and current_state != 0:
    9          current_state = AdjList[current_state]["fail_state"]
    10      current_state = find_next_state(current_state, line[i])
    11      if current_state is None:
    12          current_state = 0
    13      else:
    14          for j in AdjList[current_state]["output"]:
    15              keywords_found.append({"index":i-len(j) + 1,"word":j})
    16  return keywords_found
    Yay! We are done!

    Test it like so:

    1 init_trie(['cash', 'shew', 'ew'])
    2 print get_keywords_found("cashew")
    As always, leave questions and concerns in the comments below. See you next time!



#########################################################################
########################################################################3

Aho-Corasick APPLICATIONS:

    Applications

    Find all strings from a given set in a text
        Given a set of strings and a text. We have to print all occurrences of all strings 
        from the set in the given text in O(len+ans), where len is the 
        length of the text and ans is the size of the answer.

        We construct an automaton for this set of strings. We will now process the 
        text letter by letter, transitioning during the different states. Initially 
        we are at the root of the trie. If we are at any time at state v, and the 
        next letter is c, then we transition to the next state with go(v,c), thereby 
        either increasing the length of the current match substring by 1, 
        or decreasing it by following a suffix link.

        How can we find out for a state v, if there are any matches with strings for 
        the set? First, it is clear that if we stand on a leaf vertex, then 
        the string corresponding to the vertex ends at this position in the text. 
        However this is by no means the only possible case of achieving a match: 
        if we can reach one or more leaf vertices by moving along the suffix links, 
        then there will be also a match corresponding to each found leaf vertex. 
        A simple example demonstrating this situation can be creating using the 
        set of strings {dabce,abc,bc} and the text dabc.

        Thus if we store in each leaf vertex the index of the string corresponding 
        to it (or the list of indices if duplicate strings appear in the set), 
        then we can find in O(n) time the indices of all strings which match 
        the current state, by simply following the suffix links from the current 
        vertex to the root. However this is not the most efficient solution, 
        since this gives us O(n len) complexity in total. However this can 
        be optimized by computing and storing the nearest leaf vertex that 
        is reachable using suffix links (this is sometimes called the exit link). 
        This value we can compute lazily in linear time. Thus for 
        each vertex we can advance in O(1) time to the next marked 
        vertex in the suffix link path, i.e. to the next match. Thus for each 
        match we spend O(1) time, and therefore we reach the complexity O(len+ans).

        If you only want to count the occurrences and not find the indices themselves, 
        you can calculate the number of marked vertices in the suffix link path for each 
        vertex v. This can be calculated in O(n) time in total. Thus we can sum up all matches in O(len).

    Finding the lexicographical smallest string of a given length 
    that doesn't match any given strings
        A set of strings and a length L is given. We have to find a 
        string of length L, which does not contain any of the string, 
        and derive the lexicographical smallest of such strings.

        We can construct the automaton for the set of strings. Let's remember, 
        that the vertices from which we can reach a leaf vertex are the states, 
        at which we have a match with a string from the set. Since in this task 
        we have to avoid matches, we are not allowed to enter such states. 
        On the other hand we can enter all other vertices. Thus we delete all 
        "bad" vertices from the machine, and in the remaining graph of the automaton 
        we find the lexicographical smallest path of length L. This task can be solved 
        in O(L) for example by depth first search.

    Finding the shortest string containing all given strings
        Here we use the same ideas. For each vertex we store a mask that 
        denotes the strings which match at this state. Then the problem can 
        be reformulated as follows: initially being in the state (v=root, mask=0), 
        we want to reach the state (v, mask=(2^n − 1) ), where n is the number of strings 
        in the set. When we transition from one state to another using a letter, 
        we update the mask accordingly. By running a breath first search we 
        can find a path to the state (v, mask=2^n−1) with the smallest length.

    Finding the lexicographical smallest string of length L containing k strings
        As in the previous problem, we calculate for each vertex the number of 
        matches that correspond to it (that is the number of marked vertices 
        reachable using suffix links). We reformulate the problem: the current 
        state is determined by a triple of numbers (v, len, cnt), and we 
        want to reach from the state (root, 0, 0) the state (v, L, k), 
        where v can be any vertex. Thus we can find such a path using depth 
        first search (and if the search looks at the edges in their 
        natural order, then the found path will automatically 
        be the lexicographical smallest).

    Problems
    UVA #11590 - Prefix Lookup
    UVA #11171 - SMS
    UVA #10679 - I Love Strings!!
    Codeforces - x-prime Substrings
    Codeforces - Frequency of String
    CodeChef - TWOSTRS

################################################################
################################################################3

Z-algorithm easy explanation:
(https://cp-algorithms.com/string/z-function.html)

    Z-function and its calculation

    Suppose we are given a string s of length n. The Z-function for this string 
    is an array of length n where the i-th element is equal to the greatest 
    number of characters starting from the position i that 
    coincide with the first characters of s.

    In other words, z[i] is the length of the longest common prefix 
    between s and the suffix of s starting at i.

    Note. In this article, to avoid ambiguity, we assume 0-based indexes; 
    that is: the first character of s has index 0 and the last one has index n−1.

    The first element of Z-function, z[0], is generally not well defined. 
    In this article we will assume it is zero (although it doesn't 
    change anything in the algorithm implementation).

    This article presents an algorithm for calculating the Z-function 
    in O(n) time, as well as various of its applications.

    Examples
    For example, here are the values of the Z-function computed for different strings:

    "aaaaa" - [0,4,3,2,1]
    "aaabaab" - [0,2,1,0,2,1,0]
    "abacaba" - [0,0,1,0,3,0,1]

    Trivial algorithm
    Formal definition can be represented in the following elementary O(n2) implementation.

    vector<int> z_function_trivial(string s) {
        int n = (int) s.length();
        vector<int> z(n);
        for (int i = 1; i < n; ++i)
            while (i + z[i] < n && s[z[i]] == s[i + z[i]])
                ++z[i];
        return z;
    }

    We just iterate through every position i and update z[i] for each one of them, 
    starting from z[i]=0 and incrementing it as long as we don't find 
    a mismatch (and as long as we don't reach the end of the line).

    Of course, this is not an efficient implementation. We will 
    now show the construction of an efficient implementation.


Efficient algorithm to compute the Z-function
    To obtain an efficient algorithm we will compute the values of z[i] in 
    turn from i=1 to n−1 but at the same time, when computing a new value, 
    we'll try to make the best use possible of the previously computed values.

    For the sake of brevity, let's call segment matches those substrings that 
    coincide with a prefix of s. For example, the value of the desired 
    Z-function z[i] is the length of the segment match starting at 
    position i (and that ends at position i+z[i]−1).

    To do this, we will keep the [l,r] indices of the rightmost segment match. 
    That is, among all detected segments we will keep the one that ends 
    rightmost. In a way, the index r can be seen as the "boundary" to 
    which our string s has been scanned by the algorithm; 
    everything beyond that point is not yet known.

    Then, if the current index (for which we have to compute the next 
    value of the Z-function) is i, we have one of two options:

    i>r -- the current position is outside of what we have already processed.

    We will then compute z[i] with the trivial algorithm (that is, just comparing 
    values one by one). Note that in the end, if z[i]>0, we'll have to 
    update the indices of the rightmost segment, because it's 
    guaranteed that the new r=i+z[i]−1 is better than the previous r.

    i≤r -- the current position is inside the current segment match [l,r].

    Then we can use the already calculated Z-values to "initialize" the value 
    of z[i] to something (it sure is better than "starting from zero"), 
    maybe even some big number.

    For this, we observe that the substrings s[l…r] and s[0…r−l] match. 
    This means that as an initial approximation for z[i] we can take 
    the value already computed for the corresponding segment s[0…r−l], 
    and that is z[i−l].

    However, the value z[i−l] could be too large: when applied 
    to position i it could exceed the index r. This is not allowed 
    because we know nothing about the characters 
    to the right of r: they may differ from those required.

    Here is an example of a similar scenario:

    s="aaaabaa"
    When we get to the last position (i=6), the current match 
    segment will be [5,6]. Position 6 will then match position 6−5=1, 
    for which the value of the Z-function is z[1]=3. Obviously, we 
    cannot initialize z[6] to 3, it would be completely incorrect. 
    The maximum value we could initialize it to is 1 -- because it's the 
    largest value that doesn't bring us beyond the index r of the match segment [l,r].

    Thus, as an initial approximation for z[i] we can safely take:

    z0[i]=min(r−i+1,z[i−l])
    After having z[i] initialized to z0[i], we try to increment z[i] by 
    running the trivial algorithm -- because in general, after the border r, 
    we cannot know if the segment will continue to match or not.

    Thus, the whole algorithm is split in two cases, which differ only in 
    the initial value of z[i]: in the first case it's assumed to be zero, 
    in the second case it is determined by the previously computed 
    values (using the above formula). After that, both branches of this 
    algorithm can be reduced to the implementation of the trivial algorithm,
    which starts immediately after we specify the initial value.

    The algorithm turns out to be very simple. Despite the fact that on 
    each iteration the trivial algorithm is run, we have made significant 
    progress, having an algorithm that runs in linear time. Later on we will 
    prove that the running time is linear.

    Implementation
    Implementation turns out to be rather laconic:

    vector<int> z_function(string s) {
        int n = (int) s.length();
        vector<int> z(n);
        for (int i = 1, l = 0, r = 0; i < n; ++i) {
            if (i <= r)
                z[i] = min (r - i + 1, z[i - l]);
            while (i + z[i] < n && s[z[i]] == s[i + z[i]])
                ++z[i];
            if (i + z[i] - 1 > r)
                l = i, r = i + z[i] - 1;
        }
        return z;
    }

    Comments on this implementation
    The whole solution is given as a function which returns an array of length n -- the Z-function of s.

    Array z is initially filled with zeros. The current rightmost match segment 
    is assumed to be [0;0] (that is, a deliberately small segment which doesn't contain any i).

    Inside the loop for i=1…n−1 we first determine the initial value z[i] -- 
        it will either remain zero or be computed using the above formula.

    Thereafter, the trivial algorithm attempts to increase the value of z[i] as much as possible.

    In the end, if it's required (that is, if i+z[i]−1>r), we update the rightmost match segment [l,r].


###################################################################
#####################################################################
z Algorithm Applications:
(https://cp-algorithms.com/string/z-function.html)

    Applications
    We will now consider some uses of Z-functions for specific tasks.

    These applications will be largely similar to applications of prefix function.

    Search the substring
    To avoid confusion, we call t the string of text, and p the pattern. 
    The problem is: find all occurrences of the pattern p inside the text t.

    To solve this problem, we create a new string s=p+⋄+t, that is, we apply 
    string concatenation to p and t but we also put a separator character ⋄ 
    in the middle (we'll choose ⋄ so that it will certainly not be 
    present anywhere in the strings p or t).

    Compute the Z-function for s. Then, for any i in the interval [0;length(t)−1], 
    we will consider the corresponding value k=z[i+length(p)+1]. If k is equal 
    to length(p) then we know there is one occurrence of p in the i-th position 
    of t, otherwise there is no occurrence of p in the i-th position of t.

    The running time (and memory consumption) is O(length(t)+length(p)).

    Number of distinct substrings in a string
    Given a string s of length n, count the number of distinct substrings of s.

    We'll solve this problem iteratively. That is: knowing the current 
    number of different substrings, recalculate this amount after adding to the end of s one character.

    So, let k be the current number of distinct substrings of s. We 
    append a new character c to s. Obviously, there can be some new 
    substrings ending in this new character c (namely, all those strings 
    that end with this symbol and that we haven't encountered yet).

    Take a string t=s+c and invert it (write its characters in reverse order). 
    Our task is now to count how many prefixes of t are not found anywhere else in t. 
    Let's compute the Z-function of t and find its maximum value zmax. Obviously, 
    t's prefix of length zmax occurs also somewhere in the middle of t. Clearly, 
    shorter prefixes also occur.

    So, we have found that the number of new substrings that appear when symbol c is
    appended to s is equal to length(t)−zmax.

    Consequently, the running time of this solution is O(n2) for a string of length n.

    It's worth noting that in exactly the same way we can recalculate, still in 
    O(n) time, the number of distinct substrings when appending a character in the 
    beginning of the string, as well as when removing it (from the end or the beginning).

    String compression
    Given a string s of length n. Find its shortest "compressed" representation, 
    that is: find a string t of shortest length such that s can be represented 
    as a concatenation of one or more copies of t.

    A solution is: compute the Z-function of s, loop through all i such that i 
    divides n. Stop at the first i such that i+z[i]=n. Then, the string s 
    can be compressed to the length i.

    The proof for this fact is the same as the solution which uses the prefix function.




#########################################################################
########################################################################3
Z algorithm (https://www.hackerearth.com/practice/algorithms/string-algorithm/z-algorithm/tutorial/)

COOL NOTES: Z ALGORITHM FOR STRINGS:


    The Z-function for a string S of length N is an array of length N 
    where the i th element is equal to the greatest number of 
    characters starting from the position i 
    that coincide with the first characters of S.

    In other words, z[i] is the length 
    of the longest common prefix between S 
    and the suffix of S starting at i. We assume 0-based indexes; 
    that is, the first character of S has index 0 and the last one has index N-1.

    The first element of Z-functions, z[0], is generally not 
    well-defined. In this article we will assume it is zero.

    z [ 0 ] = 0

    This article presents an algorithm for calculating the Z-function 
    in O(N) time, as well as various of its applications.


    Examples

    For example, here are the values of the Z-function computed for different strings:

    s = 'aaaaa'
    Z[0]	Z[1]	Z[2]	Z[3]	Z[4]
    0	      4	      3	      2	      1

    s = 'aaabaab'
    Z[0]	Z[1]	Z[2]	Z[3]	Z[4]	Z[5]	Z[6]
    0	      2	      1	      0	      2	      1	      0

    s = 'abacaba'
    Z[0]	Z[1]	Z[2]	Z[3]	Z[4]	Z[5]	Z[6]
    0	      0	      1	      0	      3       0	      1

    Trivial algorithm

    The formal definition can be represented in the following elementary implementation.

    vector<int> z_function_trivial(string s) 
    {
        int n = (int) s.length();
        vector<int> z(n);
        for (int i = 1; i < n; ++i)
            while (i + z[i] < n && s[z[i]] == s[i + z[i]])
                ++z[i];
        return z;
    }

    We just iterate through every position and update for each one of them, 
    starting from and incrementing it as long as we do not 
    find a mismatch (and as long as we do not reach the end of the line).

    Efficient algorithm

    The idea is to maintain an interval [L, R] which is the interval with max R
    such that [L,R] is prefix substring (substring which is also prefix). 

    Steps for maintaining this interval are as follows – 

    1) If i > R then there is no prefix substring that starts before i and 
    ends after i, so we reset L and R and compute new [L,R] by comparing 
    str[0..] to str[i..] and get Z[i] (= R-L+1).

    2) If i <= R then let K = i-L,  now Z[i] >= min(Z[K], R-i+1)  because 
    str[i..] matches with str[K..] for atleast R-i+1 characters (they are in
    [L,R] interval which we know is a prefix substring).     
    Now two sub cases arise – 
        a) If Z[K] < R-i+1  then there is no prefix substring starting at 
            str[i] (otherwise Z[K] would be larger)  so  Z[i] = Z[K]  and 
            interval [L,R] remains same.
        b) If Z[K] >= R-i+1 then it is possible to extend the [L,R] interval
            thus we will set L as i and start matching from str[R]  onwards  and
            get new R then we will update interval [L,R] and calculate Z[i] (=R-L+1).

            
    Implementation
    // returns array z[] where z[i] is z-function of s[i]
    int[] zFucntion(String s) {
        int n = s.length();
        int z[] = new int[n];
        int R = 0;
        int L = 0;
        for(int i = 1; i < n; i++) {
            z[i] = 0;
            if (R > i) {
                z[i] = Math.min(R - i, z[i - L]);
            }
            while (i + z[i] < n && s.charAt(i+z[i]) == s.charAt(z[i])) {
                z[i]++;
            }
            if (i + z[i] > R) {
                L = i;
                R = i + z[i];
            }
        }
        z[0] = n;
        return z;
    }

    Complexity
    Worst case time complexity: Θ(N)
    Average case time complexity: Θ(N)
    Best case time complexity: Θ(N)
    Space complexity: Θ(log N)

    Applications
    PLEASE COVER:
        Boyer moore good character heuristic/bad char heuristic
        Aho-Corasick Algorithm for Pattern Searching
        Suffix Tree/Suffix Array
        Manachars algorithm (https://www.hackerearth.com/practice/algorithms/string-algorithm/manachars-algorithm/tutorial/)

    Applications of Z algorithms are as follows:
    Finding all occurrences of the pattern P inside the text T in O(length(T) + length(P))
    Counting the number of distinct substrings of a string S in O(1)
    Finding a string T of shortest length such that S can be represented as a 
    concatenation of one or more copies of T




###################################################################################################
Segment trees are soooo cool. 

    Basically, if you have some list, and you want to calculate.

    reduce(operation, arr[i:j], default_value)

    And you want to do this repeatedly. You can have O(n) for this operation 
    every time and O(1) for updating the array, or you can have O(log(n)) to 
    calculate this value and O(log(n)) to update using a Segment Tree.

    class SegmentTree:
        
        def __init__(self, arr, operator, default):
            self.n = len(arr)
            self.default = default
            self.arr = [0 for _ in range(2 * len(arr) - 1)]
            self.operator = operator
            
            for i in range(len(arr) - 1, len(self.arr)):
                self.arr[i] = arr[i - len(arr) + 1]

            for i in range(len(arr) - 2, -1, -1):
                self.arr[i] = self.operator(self.arr[i * 2 + 1], self.arr[i * 2 + 2])

        def update(self, idx, value):
            idx = self.n - 1 + idx
            self.arr[idx] = value
            while idx > 0:
                idx = (idx - 1)/2
                self.arr[idx] = self.operator(self.arr[idx * 2 + 1], self.arr[idx * 2 + 2])

        def get_segment_value(self, left, right):
            left = self.n - 1 + left
            right = self.n - 1 + right
            ans = self.default
            while left < right:
                if left % 2 == 0:
                    ans = self.operator(ans, self.arr[left])
                    left += 1
                if right % 2 == 0:
                    right -= 1
                    ans = self.operator(ans, self.arr[right])
                left, right = (left - 1)/2, (right - 1)/2
            return ans

####################################################################
Segment trees Analysis and in C++

    Let our data be in an array arr[] of size nn.

    The root of our segment tree typically represents the entire interval of data 
    we are interested in. This would be arr[0:n-1].
    
    -> Each leaf of the tree represents a range comprising of just a single element. 
    -> Thus the leaves represent arr[0], arr[1] and so on till arr[n-1].
    -> The internal nodes of the tree would represent the merged or union result of their children nodes.
    -> Each of the children nodes could represent approximately half of the range represented by their parent.
    -> A segment tree for an n element range can be comfortably represented using an array of size n≈4∗n.
    PROOF:
        -> What is happening here is, if you have an array of n elements, 
        then the segment tree will have a leaf node for each of these n entries. 
        Thus, we have (n) leaf nodes, and also (n-1) internal nodes.
        -> Total number of nodes= n + (n-1) = 2n-1 
        -> Now, we know its a full binary tree and thus the height is: ceil(Log2(n)) +1
        -> Total no. of nodes = 2^0 + 2^1 + 2^2 + … + 2^ceil(Log2(n)) 
        // which is a geometric progression where 2^i denotes, the number of nodes at level i.
        -> Formula of summation G.P. = a * (r^size - 1)/(r-1) where a=2^0
        -> Total no. of nodes = 1*(2^(ceil(Log2(n))+1) -1)/(2-1)
        = 2* [2^ceil(Log2(n))] -1 (you need space in the array for each of the internal 
        as well as leaf nodes which are this count in number), thus it is the array of size.
        = O(4 * n) approx..
    
    EASIER PROOF:
        Also, the better explanation is: if the array size n is a power of 2, 
        then we have exactly n-1 internal nodes, summing up to 2n-1 total nodes. 
        But not always, we have n as the power of 2, so we basically 
        need the smallest power of 2 which is greater than n. That means this,
        int s=1;
        for(; s<n; s<<=1);

    The node of the tree is at index 0. Thus tree[0] is the root of our tree.
    The children of tree[i] are stored at tree[2*i+1] and tree[2*i+2].

    1. Build the tree from the original data.
    void buildSegTree(vector<int>& arr, int treeIndex, int lo, int hi)
    {
        if (lo == hi) {                 // leaf node. store value in node.
            tree[treeIndex] = arr[lo];
            return;
        }

        int mid = lo + (hi - lo) / 2;   // recurse deeper for children.
        buildSegTree(arr, 2 * treeIndex + 1, lo, mid);
        buildSegTree(arr, 2 * treeIndex + 2, mid + 1, hi);

        // merge build results
        tree[treeIndex] = merge(tree[2 * treeIndex + 1], tree[2 * treeIndex + 2]);
    }

    // call this method as buildSegTree(arr, 0, 0, n-1);
    // Here arr[] is input array and n is its size.

    The method builds the entire tree in a bottom up fashion. When the condition 
    lo = hi is satisfied, we are left with a range comprising of 
    just a single element (which happens to be arr[lo]). 
    This constitutes a leaf of the tree. The rest of the 
    nodes are built by merging the results of their two children.
    treeIndex is the index of the current node of the segment tree which is being processed.

    Input -> 10 nodes -> arr[] = { 18, 17, 13, 19, 15, 11, 20, 12, 33, 25 };
    tree[] = { 183, 82, 101, 48, 34, 43, 58, 35, 13, 19, 15, 31, 12, 33, 
               25, 18, 17, 0, 0, 0, 0, 0, 0, 11, 20, 0, 0, 0, 0, 0, 0 };

    Notice the the groups of zeros near the end of the tree[] array? 
    Those are null values we used as padding to ensure a complete binary tree 
    is formed (since we only had 1010 leaf elements. Had we had, say, 
    16 leaf elements, we wouldn't need any null element

    NOTE: The merge operation varies from problem to problem. 
    You should closely think of what to store in a node of the segment 
    tree and how two nodes will merge to provide a result before you even 
    start building a segment tree.

    2. Read/Query on an interval or segment of the data.
    int querySegTree(int treeIndex, int lo, int hi, int i, int j)
    {
        // query for arr[i..j]

        if (lo > j || hi < i)               // segment completely outside range
            return 0;                       // represents a null node

        if (i <= lo && j >= hi)             // segment completely inside range
            return tree[treeIndex];
        
        // partial overlap of current segment and queried range. Recurse deeper.
        int mid = lo + (hi - lo) / 2;       

        if (i > mid)
            return querySegTree(2 * treeIndex + 2, mid + 1, hi, i, j);
        else if (j <= mid)
            return querySegTree(2 * treeIndex + 1, lo, mid, i, j);

        // When mid is between i and j. Find the range by traversing left and right subtrees
        // but now look for intervals [i, mid] and [mid+1, j] which is [i, j]

        int leftQuery = querySegTree(2 * treeIndex + 1, lo, mid, i, mid);
        int rightQuery = querySegTree(2 * treeIndex + 2, mid + 1, hi, mid + 1, j);

        // merge query results, for instance sum.
        return merge(leftQuery, rightQuery);
    }

    // call this method as querySegTree(0, 0, n-1, i, j);
    // Here [i,j] is the range/interval you are querying.
    // This method relies on "null" nodes being equivalent to storing zero.

    In the above example, we are trying to find the sum of 
    the elements in the range [2, 8] . No segment completely 
    represents the range [2, 8]. 
    However we can see that [2, 8] can be built up using the ranges 
    [2, 2], [3, 4], [5, 7] and [8, 8]. 
     
    void updateValSegTree(int treeIndex, int lo, int hi, int arrIndex, int val)
    {
        if (lo == hi) {                 // leaf node. update element.
            tree[treeIndex] = val;
            return;
        }

        int mid = lo + (hi - lo) / 2;   // recurse deeper for appropriate child

        if (arrIndex > mid)
            updateValSegTree(2 * treeIndex + 2, mid + 1, hi, arrIndex, val);
        else if (arrIndex <= mid)
            updateValSegTree(2 * treeIndex + 1, lo, mid, arrIndex, val);

        // merge updates, and update the upper levels of the tree straight to the root.
        tree[treeIndex] = merge(tree[2 * treeIndex + 1], tree[2 * treeIndex + 2]);
    }
    
    This makes the build process run in O(n) linear complexity.
    Both the read and update queries now take logarithmic O(log_2(n))
##########################################################################################3
PYTHON SEGMENT TREE FOR ABOVE:

class NumArray:
    tree = None
    n = None
    def __init__(self, nums: List[int]):
        self.n = len(nums)
        if self.n:
            self.tree = [0] * 4 * len(nums)
            self.buildTree(nums, 0, 0, len(nums) - 1)

    '''
    arr: The original array
    index: the Segment tree index
    low: the lower bound
    high: the upper bound
    '''
    def buildTree(self, arr, index, low, high):
        if low == high:
            self.tree[index] = arr[low] if low < len(arr) else 0
        else:
            mid = (low + high) // 2
            self.buildTree(arr, 2 * index + 1, low, mid)
            self.buildTree(arr, 2 * index + 2, mid + 1, high)
            self.tree[index] = self.tree[2 * index + 1] + self.tree[2 * index + 2]
    
    '''
    i: the index that needs to be updated
    val: the value been updated
    index: the index of segment tree
    low: the lower bound
    high: the higher bound
    '''
    def update(self, i: int, val: int, index = 0, low = 0, high = float('inf')) -> None:
        if high == float('inf'):
            high = self.n - 1
        if low == high:
            self.tree[index] = val
        else:
            mid = (low + high) // 2
            if mid >= i:
                self.update(i, val, 2 * index + 1, low, mid)
            else:
                self.update(i, val , 2 * index + 2, mid + 1, high)
            self.tree[index] = self.tree[2 * index + 1] + self.tree[2 * index + 2]

    '''
    i: the lower bound of the query
    j: the upper bound of the query
    '''
    def sumRange(self, i: int, j: int, index = 0, low = 0, high = float('inf')) -> int:
        if high == float('inf'):
            high = self.n - 1
        if low > j or high < i:
            return 0
        if i <= low and j >= high:
            return self.tree[index]
        mid = (low + high) // 2
        if i > mid:
            return self.sumRange(i, j, 2 * index + 2, mid + 1, high)
        elif j <= mid:
            return self.sumRange(i, j, 2 * index + 1, low, mid)
        left = self.sumRange(i, mid , 2 * index + 1, low, mid)
        right = self.sumRange(mid + 1, j , 2 * index + 2, mid + 1, high)
        return left + right    

###########################################################################################
LAZY PROPOGATION SEGMENT TREE:

    Till now we have been updating single elements only. 
    That happens in logarithmic time and it's pretty efficient.
    But what if we had to update a range of elements? By our current method, 
    each of the elements would have to be updated 
    independently, each incurring some run time cost.
    The construction of a tree poses another issue called ancestral locality. 
    Ancestors of adjacent leaves are guaranteed to be common at 
    some levels of the tree. Updating each of these leaves individually 
    would mean that we process their common ancestors multiple times. 
    What if we could reduce this repetitive computation?
    A third kind of problem is when queried ranges do not contain frequently updated elements. 
    We might be wasting valuable time updating nodes which are rarely going to be accessed/read.

    TO IMPLEMENT:
    We use another array lazy[] which is the same size as our segment tree 
    array tree[] to represent a lazy node. lazy[i] holds the amount by which 
    the node tree[i] needs to be incremented, when that node is finally accessed 
    or queried. When lazy[i] is zero, it means that node 
    tree[i] is not lazy and has no pending updates.

    1. Updating a range lazily
    This is a three step process:

    -> Normalize the current node. This is done by removing laziness. 
       We simple increment the current node by appropriate amount to 
       remove it's laziness. Then we mark its children to be lazy as 
       the descendants haven't been processed yet.
    
    -> Apply the current update operation to the current node if the 
       current segment lies inside the update range.

    -> Recurse for the children as you would normally to find appropriate segments to update.

    void updateLazySegTree(int treeIndex, int lo, int hi, int i, int j, int val)
    {
        if (lazy[treeIndex] != 0) {                             // this node is lazy
            tree[treeIndex] += (hi - lo + 1) * lazy[treeIndex]; // normalize current node by removing laziness

            if (lo != hi) {                                     // update lazy[] for children nodes
                lazy[2 * treeIndex + 1] += lazy[treeIndex];
                lazy[2 * treeIndex + 2] += lazy[treeIndex];
            }

            lazy[treeIndex] = 0;                                // current node processed. No longer lazy
        }

        if (lo > hi || lo > j || hi < i)
            return;                                             // out of range. escape.

        if (i <= lo && hi <= j) {                               // segment is fully within update range
            tree[treeIndex] += (hi - lo + 1) * val;             // update segment

            if (lo != hi) {                                     // update lazy[] for children
                lazy[2 * treeIndex + 1] += val;
                lazy[2 * treeIndex + 2] += val;
            }

            return;
        }

        int mid = lo + (hi - lo) / 2;                             // recurse deeper for appropriate child

        updateLazySegTree(2 * treeIndex + 1, lo, mid, i, j, val);
        updateLazySegTree(2 * treeIndex + 2, mid + 1, hi, i, j, val);

        // merge updates
        tree[treeIndex] = tree[2 * treeIndex + 1] + tree[2 * treeIndex + 2];
    }
    // call this method as updateLazySegTree(0, 0, n-1, i, j, val);
    // Here you want to update the range [i, j] with value val.

    2. Querying a lazily propagated tree
    This is a two step process:
    Normalize the current node by removing laziness. This step is the same as the update step.
    Recurse for the children as you would normally to find appropriate segments which fit in queried range.

        int queryLazySegTree(int treeIndex, int lo, int hi, int i, int j)
    {
        // query for arr[i..j]

        if (lo > j || hi < i)                                   // segment completely outside range
            return 0;                                           // represents a null node

        if (lazy[treeIndex] != 0) {                             // this node is lazy
            tree[treeIndex] += (hi - lo + 1) * lazy[treeIndex]; // normalize current node by removing laziness

            if (lo != hi) {                                     // update lazy[] for children nodes
                lazy[2 * treeIndex + 1] += lazy[treeIndex];
                lazy[2 * treeIndex + 2] += lazy[treeIndex];
            }

            lazy[treeIndex] = 0;                                // current node processed. No longer lazy
        }

        if (i <= lo && j >= hi)                                 // segment completely inside range
            return tree[treeIndex];
        
        // partial overlap of current segment and queried range. Recurse deeper.
        int mid = lo + (hi - lo) / 2;                           

        if (i > mid)
            return queryLazySegTree(2 * treeIndex + 2, mid + 1, hi, i, j);
        else if (j <= mid)
            return queryLazySegTree(2 * treeIndex + 1, lo, mid, i, j);

        int leftQuery = queryLazySegTree(2 * treeIndex + 1, lo, mid, i, mid);
        int rightQuery = queryLazySegTree(2 * treeIndex + 2, mid + 1, hi, mid + 1, j);

        // merge query results
        return leftQuery + rightQuery;
    }
    // call this method as queryLazySegTree(0, 0, n-1, i, j);
    // Here [i,j] is the range/interval you are querying.
    // This method relies on "null" nodes being equivalent to storing zero.
    

########################## DESIGN CIRCULAR BIDIRECTIONAL LINKED LIST PYTHON ############################

class Node:
    def __init__(self, v, p=None, n=None):
        self.val = v
        self.prev = p
        self.next = n

class MyLinkedList:

    def __init__(self):
        """
        Initialize your data structure here.
        """
        self.key = Node(-1)
        self.key.prev = self.key.next = self.key

    def get(self, index: int) -> int:
        """
        Get the value of the index-th node in the linked list. 
        If the index is invalid, return -1.

        """
        i, node = 0, self.key.next
        while i < index and node != self.key:
            node = node.next
            i += 1
        return node.val if index >= 0 else -1

    def addAtHead(self, val: int) -> None:
        """
        Add a node of value val before the first element of the linked list. 
        After the insertion, the new node will be the first node of the linked list.
        """
        self.key.next.prev = self.key.next = Node(val, p=self.key, n=self.key.next)

    def addAtTail(self, val: int) -> None:
        """
        Append a node of value val to the last element of the linked list.
        """
        self.key.prev.next = self.key.prev = Node(val, p=self.key.prev, n=self.key)

    def addAtIndex(self, index: int, val: int) -> None:
        """
        Add a node of value val before the index-th node in the linked list. 
        If index equals to the length of linked list, the node will be appended 
        to the end of linked list. If index is greater than the length, the node will not be inserted.
        """
        index = max(0, index)
        i, node = 0, self.key.next
        while i < index and node != self.key:
            node = node.next
            i += 1
        if node != self.key or i == index:
            node.prev.next = node.prev = Node(val, p=node.prev, n=node)

    def deleteAtIndex(self, index: int) -> None:
        """
        Delete the index-th node in the linked list, if the index is valid.
        """
        if index < 0: return
        i, node = 0, self.key.next
        while i < index and node != self.key:
            node = node.next
            i += 1
        if node != self.key:
            node.prev.next = node.next
            node.next.prev = node.prev
            del node


# Your MyLinkedList object will be instantiated and called as such:
# obj = MyLinkedList()
# param_1 = obj.get(index)
# obj.addAtHead(val)
# obj.addAtTail(val)
# obj.addAtIndex(index,val)
# obj.deleteAtIndex(index)


########################## DESIGN CIRCULAR BIDIRECTIONAL LINKED LIST C++ ############################
class MyLinkedList {
    struct Node {
        Node* prev;
        Node* next;
        int val;
    };

    Node head;

    Node* addAfter(Node* position, int val) {
        const auto node = new Node{position, position->next, val};
        position->next->prev = node;
        position->next = node;
        return node;
    }

    Node* addBefore(Node* position, int val) {
        const auto node = new Node{position->prev, position, val};
        position->prev->next = node;
        position->prev = node;
        return node;
    }

    void remove(Node* node) {
        const auto prev = node->prev;
        const auto next = node->next;
        node->prev->next = next;
        node->next->prev = prev;
        delete node;
    }

    pair<Node*, int> at(int index) {
        Node* node = &head;
        index++;
        while (index > 0) {
            index--;
            node = node->next;
            if (node == &head) {
                break;
            }
        }
        return {node, index};
    }

public:
    /** Initialize your data structure here. */
    MyLinkedList() {
        head = {&head, &head, -1};
    }

    /** Get the value of the index-th node in the linked list. If the index is invalid, return -1. */
    int get(int index) {
        return at(index).first->val;
    }

    /** Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. */
    void addAtHead(int val) {
        addAfter(&head, val);
    }

    /** Append a node of value val to the last element of the linked list. */
    void addAtTail(int val) {
        addBefore(&head, val);
    }

    /** Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted. */
    void addAtIndex(int index, int val) {
        const auto [node, excess] = at(index);
        if (excess == 0) {
            addBefore(node, val);
        }
    }

    /** Delete the index-th node in the linked list, if the index is valid. */
    void deleteAtIndex(int index) {
        const auto node = at(index).first;
        if (node != &head) {
            remove(node);
        }
    }
};

##########################################################################################

MinHeap Implementation

    Maxheap can be implemented similarly, mainly 
    just know the heapify algorithm and you should be fine.

    class MinHeap:
        def __init__(self):
            self.heap = []
        def parent(self, i):
            return (i - 1)/2
        def left_child(self, i):
            return (i * 2 + 1)
        def right_child(self, i):
            return (i * 2 + 2)
        def insertKey(self, k):
            self.heap.append(k)
            i = len(self.heap) - 1
            while i != 0 and self.heap[self.parent(i)] > self.heap[i]:
                self.heap[i], self.heap[self.parent(i)] = self.heap[self.parent(i)], self.heap[i]
                i = self.parent(i)
        def decreaseKey(self, i, new_val):
            self.heap[i] = new_val
            while i != 0 and self.heap[self.parent(i)] > self.heap[i]:
                self.heap[i], self.heap[self.parent(i)] = self.heap[self.parent(i)], self.heap[i]
                i = self.parent(i)
        def increaseKey(self, i, new_val):
            heap[i] = new_val
            minHeapify(i)
        def getMin(self):
            return self.heap[0]
        def minHeapify(self, i):
            left_child = self.left_child(i)
            right_child = self.right_child(i)
            smallest = i
            if left_child < len(self.heap) and self.heap[left_child] < self.heap[smallest]:
                smallest = left_child
            if right_child < len(self.heap) and self.heap[right_child] < self.heap[smallest]:
                smallest = right_child
            if smallest != i:
                self.heap[i], self.heap[smallest] = self.heap[smallest], self.heap[i]
                self.minHeapify(smallest)
        def extractMin(self):
            tmp = self.heap[0]
            self.heap[0] = self.heap[-1]
            self.heap.pop()
            self.minHeapify(0)
            return tmp
        def deleteKey(self, i):
            self.decreaseKey(i, float('inf') * -1)
            self.extractMin()


############################## BUILD HEAP AND WHY ITS O(N) (Using MAX HEAP HERE)############################

Making the correct choice between siftUp and siftDown is critical 
to get O(n) performance for buildHeap

siftDown swaps a node that is too small with its largest child (thereby moving it down) 
until it is at least as large as both nodes below it.

siftUp swaps a node that is too large with its parent (thereby moving it up) 
until it is no larger than the node above it.

The number of operations required for siftDown and siftUp is proportional 
to the distance the node may have to move. For siftDown, it is the distance 
to the bottom of the tree, so siftDown is expensive for nodes at the top 
of the tree. With siftUp, the work is proportional to the distance to the 
top of the tree, so siftUp is expensive for nodes at the bottom of the tree. 
Although both operations are O(log n) in the worst case, in a heap, only one 
node is at the top whereas half the nodes lie in the bottom layer. So it 
shouldn't be too surprising that if we have to apply an operation to every node, 
we would prefer siftDown over siftUp.

Two ways:

Start at the top of the heap (the beginning of the array) and call 
siftUp on each item. At each step, the previously sifted items (the items before the current 
item in the array) form a valid heap, and sifting the next item up places it into a 
valid position in the heap. After sifting up each node, all items satisfy the heap property.

Or, go in the opposite direction: start at the end of the array and 
move backwards towards the front. At each iteration, you sift an item 
down until it is in the correct location.

Both of these solutions will produce a valid heap. Unsurprisingly, 
the more efficient one is the second operation that uses siftDown.

Let h = log n represent the height of the heap. 
The work required for the siftDown approach is given by the sum
(0 * n/2) + (1 * n/4) + (2 * n/8) + ... + (h * 1).

In contrast, the sum for calling siftUp on each node is
(h * n/2) + ((h-1) * n/4) + ((h-2)*n/8) + ... + (0 * 1).

USE SIFT DOWN!!

BUILD HEAP (BUT FOR MIN HEAP):

def heapify(A):
    for root in xrange(len(A)//2-1, -1, -1):
        rootVal = A[root]
        child = 2*root + 1
        while child < len(A):
            # we pick the smaller child to sort?
            # makes sense because the smaller child is the one
            # that has to fight with the parent in a min heap.
            if child+1 < len(A) and A[child] > A[child+1]:
                child += 1
            if rootVal <= A[child]:
                break
            A[child], A[(child-1)//2] = A[(child-1)//2], A[child]
            child = child *2 + 1


############################################ TREE ANALYSIS ######################33

Segment tree stores intervals, and optimized for "which of these intervals contains a given point" queries.
Interval tree stores intervals as well, but optimized for "which of these intervals overlap with a given interval" queries. 
        It can also be used for point queries - similar to segment tree.
Range tree stores points, and optimized for "which points fall within a given interval" queries.
Binary indexed tree stores items-count per index, and optimized for "how many items are there between index m and n" queries.

Performance / Space consumption for one dimension:

Segment tree - O(n logn) preprocessing time, O(k+logn) query time, O(n logn) space
Interval tree - O(n logn) preprocessing time, O(k+logn) query time, O(n) space
Range tree - O(n logn) preprocessing time, O(k+logn) query time, O(n) space
Binary Indexed tree - O(n logn) preprocessing time, O(logn) query time, O(n) space
(k is the number of reported results).

All data structures can be dynamic, in the sense that 
the usage scenario includes both data changes and queries:

Segment tree - interval can be added/deleted in O(logn) time (see here)
Interval tree - interval can be added/deleted in O(logn) time
Range tree - new points can be added/deleted in O(logn) time (see here)
Binary Indexed tree - the items-count per index can be increased in O(logn) time
Higher dimensions (d>1):

Segment tree - O(n(logn)^d) preprocessing time, O(k+(logn)^d) query time, O(n(logn)^(d-1)) space
Interval tree - O(n logn) preprocessing time, O(k+(logn)^d) query time, O(n logn) space
Range tree - O(n(logn)^d) preprocessing time, O(k+(logn)^d) query time, O(n(logn)^(d-1))) space
Binary Indexed tree - O(n(logn)^d) preprocessing time, O((logn)^d) query time, O(n(logn)^d) space






########################## HASH MAP AND HASH SET NOTES ###############################3

Hashing vs. Balanced Search Trees
Advantages of Balanced Search Trees
O(log n) worst-case operation cost
Does not require any assumptions, special functions,
or known properties of input distribution
No wasted space
Never need to rebuild the entire structure


Advantages of Hash Tables
O(1) cost, but only on average
Flexible load factor parameters
Cuckoo hashing achieves O(1) worst-case for search & delete

How to handle Collisions?
There are mainly two methods to handle collision:
1) Separate Chaining
2) Open Addressing

Collision Resolution
Even the best hash function may have collisions:
when we want to insert (k, v ) into the table,
but T [h(k)] is already occupied.
Two basic strategies:
Allow multiple items at each table location (buckets)
Allow each item to go into multiple locations (open addressing)

We will examine the average cost of search, insert, delete,
in terms of n, M, and/or the load factor α = n/M.

n is number of items in hash table, M is number of spots 
Requirement: For a given M ∈ N,
every key k is an integer with 0 ≤ k < M.

We probably want to rebuild the whole hash table and change
the value of M when the load factor gets too large or too small.
This is called rehashing , and should cost roughly Θ(M + n).

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
SEPERATE CHAINING VS OPEN ADDRESSING PERFORMACNE

Separate Chaining
Keys are stored inside the hash table as well as outside the hash table.	
The number of keys to be stored in the hash table can even exceed the size of the hash table.	
Deletion is easier.	
Extra space is required for the pointers to store the keys outside the hash table.	
Cache performance is poor. This is because of linked lists which store the keys outside the hash table.
Some buckets of the hash table are never used which leads to wastage of space.

Open Addressing
All the keys are stored only inside the hash table.
No key is present outside the hash table.
The number of keys to be stored in the hash table can never exceed the size of the hash table.
Deletion is difficult.
No extra space is required.
Cache performance is better.This is because here no linked lists are used.
Buckets may be used even if no key maps to those particular buckets.

Separate Chaining-
Separate Chaining is advantageous when it is required to perform
all the following operations on the keys stored in the hash table-
Insertion Operation
Deletion Operation
Searching Operation
NOTE-
Deletion is easier in separate chaining.
This is because deleting a key from the hash table does not affect the other keys stored in the hash table.
 
Open Addressing-
Open addressing is advantageous when it is required to perform only 
the following operations on the keys stored in the hash table-
Insertion Operation
Searching Operation
NOTE-
Deletion is difficult in open addressing.
This is because deleting a key from the hash table requires some extra efforts.
After deleting a key, certain keys have to be rearranged.


Seperate Chaining >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 

Each table entry is a bucket containing 0 or more KVPs.
This could be implemented by any dictionary (even another hash table!).
The simplest approach is to use an unsorted linked list in each bucket.
This is called collision resolution by chaining .

search(k): Look for key k in the list at T [h(k)].
insert(k, v ): Add (k, v ) to the front of the list at T [h(k)].
delete(k): Perform a search, then delete from the linked list.

The bucket chains are often searched sequentially using the order 
the entries were added to the bucket. If the load factor is large 
and some keys are more likely to come up than others, then rearranging 
the chain with a move-to-front heuristic may be effective. More sophisticated data structures, 
such as balanced search trees, are worth considering only if the load factor is large (about 10 or more), 
or if the hash distribution is likely to be very non-uniform, or if one must guarantee good performance 
even in a worst-case scenario. However, using a larger table and/or a 
better hash function may be even more effective in those cases.

Advantages:
1) Simple to implement.
2) Hash table never fills up, we can always add more elements to the chain.
3) Less sensitive to the hash function or load factors.
4) It is mostly used when it is unknown how many and how frequently keys may be inserted or deleted.

Disadvantages:
1) Cache performance of chaining is not good as keys are stored using a linked list. 
    Open addressing provides better cache performance as everything is stored in the same table.
2) Wastage of Space (Some Parts of hash table are never used)
3) If the chain becomes long, then search time can become O(n) in the worst case.
4) Uses extra space for links.

Implementing hash table using Chaining through Doubly Linked List is similar to 
implementing Hashtable using Singly Linked List. The only difference is that every 
node of Linked List has the address of both, the next and the previous node. This 
will speed up the process of adding and removing elements from the list, hence the 
time complexity will be reduced drastically.

Some chaining implementations store the first record of each chain in the 
slot array itself.[4] The number of pointer traversals is decreased 
by one for most cases. The purpose is to 
increase cache efficiency of hash table access.




Recall the load balance α = n/M.
Assuming uniform hashing, average bucket size is exactly α.
Analysis of operations:
search Θ(1 + α) average-case, Θ(n) worst-case
insert O(1) worst-case, since we always insert in front.
delete Same cost as search: Θ(1 + α) average, Θ(n) worst-case
If we maintain M ∈ Θ(n), then average costs are all O(1).
This is typically accomplished by rehashing whenever n < c 1 M or n > c 2 M,
for some constants c 1 , c 2 with 0 < c 1 < c 2 .


Open addressing >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>.:

Main idea: Each hash table entry holds only one item,
but any key k can go in multiple locations.
search and insert follow a probe sequence of possible locations for key k:
<h(k, 0), h(k, 1), h(k, 2), . . .>

delete becomes problematic; we must distinguish between
empty and deleted locations.


Simplest idea: linear probing
h(k, i) = (h(k) + i) mod M, for some hash function h.

SO BASICALLY, when you see a collision in hash keys, just put it in the first empty array spot that you
identify by probing linearly, increase the index by 1 each time. If we linear probe until the end of the array
wrap around and start probing from index 0 of the array. This is done when you mod it by M above to search for 
empty array spot.

When you delete something, MARK THAT ARRAY SPOT AS DELETED SO IT THE FOLLOWING REMAINS TRUE:
    During insertion, the buckets marked as “deleted” are treated like any other empty bucket.
    During searching, the search is not terminated on encountering the bucket marked as “deleted”.
    The search terminates only after the required key or an empty bucket is found.

Well-known probe sequences include:
Linear probing, in which the interval between probes is fixed (usually 1)

Quadratic probing, in which the interval between probes is increased by adding the 
successive outputs of a quadratic polynomial to the 
starting value given by the original hash computation

Double hashing, in which the interval between probes is computed by a second hash function
Double hashing is a collision resolving technique in Open Addressed Hash tables. 
Double hashing uses the idea of applying a second hash function to key when a collision occurs.

Double hashing can be done using :
(hash1(key) + i * hash2(key)) % TABLE_SIZE
Here hash1() and hash2() are hash functions and TABLE_SIZE
is size of hash table.

(We repeat by increasing i when collision occurs)
First hash function is typically hash1(key) = key % TABLE_SIZE

A popular second hash function is : hash2(key) = PRIME – (key % PRIME) 
where PRIME is a prime smaller than the TABLE_SIZE.

A good second Hash function is:
    - It must never evaluate to zero
    - Must make sure that all cells can be probed

Uniform Hashing Assumption: Each hash function value is equally likely.
Proving is usually impossible, as it requires knowledge of
the input distribution and the hash function distribution.
We can get good performance by following a few rules.
A good hash function should:
    be very efficient to compute
    be unrelated to any possible patterns in the data
    depend on all parts of the key
If all keys are integers (or can be mapped to integers),
the following two approaches tend to work well:
Division method: h(k) = k mod M.
We should choose M to be a prime.

Multiplication method: h(k) = floor(M(kA − floor(kAc)),
for some constant floating-point
number A with 0 < A < 1. -> Knuth suggests A = 0.618

What if the keys are multi-dimensional, such as strings?
Standard approach is to flatten string w to integer f (w ) ∈ N, e.g.
A · P · P · L · E
→ 65 · 80 · 80 · 76 · 69

→ 65R^4 + 80R^3 + 80R^2 + 76R^1 + 68R^0
(for some radix R, e.g. R = 255)
We combine this with a standard hash function
h : N → {0, 1, 2, . . . , M − 1}.
With h(f (k)) as the hash values, we then use any standard hash table.
Note: computing each h(f(k)) takes Ω(length of w) time

So, under uniform hashing, we assume the probability that a key k
has h1(k) = a and h2(k) = b, for any particular a and b, is 1/(M^2)
For double hashing , define h(k, i) = h1(k) + i · h2(k) mod M.
search, insert, delete work just like for linear probing,
but with this different probe sequence.


1. Linear Probing-
In linear probing,
When collision occurs, we linearly probe for the next bucket.
We keep probing until an empty bucket is found.
Advantage-
It is easy to compute.
Disadvantage-
The main problem with linear probing is clustering.
Many consecutive elements form groups.
Then, it takes time to search an element or to find an empty bucket.
 
Time Complexity-
Worst time to search an element in linear probing is O (table size).
This is because-
Even if there is only one element present and all other elements are deleted.
Then, “deleted” markers present in the hash table makes search the entire table.
 
2. Quadratic Probing-
In quadratic probing,
When collision occurs, we probe for i2‘th bucket in ith iteration.
We keep probing until an empty bucket is found.
 
3. Double Hashing-
In double hashing,
We use another hash function hash2(x) and look for i * hash2(x) bucket in ith iteration.
It requires more computation time as two hash functions need to be computed.

Linear Probing has the best cache performance but suffers from clustering.
Quadratic probing lies between the two in terms of cache performance and clustering.
Double caching has poor cache performance but no clustering.


A drawback of all these open addressing schemes is that the number of stored entries cannot 
exceed the number of slots in the bucket array. In fact, even with good hash functions, 
their performance dramatically degrades when the load factor grows beyond 0.7 or so. 
For many applications, these restrictions mandate the use of dynamic resizing, 
with its attendant costs.

Open addressing schemes also put more stringent requirements on the hash function: 
besides distributing the keys more uniformly over the buckets, the function must also 
minimize the clustering of hash values that are consecutive in the probe order. Using 
separate chaining, the only concern is that too many objects map to the same hash value; 
whether they are adjacent or nearby is completely irrelevant.[citation needed]

Open addressing only saves memory if the entries are small (less than four times the size of a pointer) 
and the load factor is not too small. If the load factor is close to zero (that is, there are far more 
buckets than stored entries), open addressing is wasteful even if each entry is just two words.

This graph compares the average number of cache misses 
required to look up elements in tables with chaining 
and linear probing. As the table passes the 80%-full mark, 
linear probing's performance drastically degrades.

Open addressing avoids the time overhead of allocating each new entry record, and 
can be implemented even in the absence of a memory allocator. It also avoids the extra 
indirection required to access the first entry of each bucket (that is, usually the only one). 
It also has better locality of reference, particularly with linear probing. With small record sizes, 
these factors can yield better performance than chaining, particularly for lookups. Hash tables with 
open addressing are also easier to serialize, because they do not use pointers.[citation needed]

On the other hand, normal open addressing is a poor choice for large elements, because these 
elements fill entire CPU cache lines (negating the cache advantage), and a large amount of space 
is wasted on large empty table slots. If the open addressing table only stores references to 
elements (external storage), it uses space comparable to chaining even for large records 
but loses its speed advantage.[citation needed]

Generally speaking, open addressing is better used for hash tables with small 
records that can be stored within the table (internal storage) and fit in a cache line. 
They are particularly suitable for elements of one word or less. If the table is expected 
to have a high load factor, the records are large, or the data is variable-sized, 
chained hash tables often perform as well or better.

>>>>>>>>>>>>>>(TODO) talk about Coalesced hashing

A hybrid of chaining and open addressing, coalesced hashing links together 
chains of nodes within the table itself.[18] Like open addressing, 
it achieves space usage and (somewhat diminished) cache advantages over 
chaining. Like chaining, it does not exhibit clustering effects; 
in fact, the table can be efficiently filled to a high density. 
Unlike chaining, it cannot have more elements than table slots.

>>>>>>>>>>>>>>>>>>> CUCKOO HASHING

This is a relatively new idea from Pagh and Rodler in 2001.
Again, we use two independent hash functions h1 , h2.
The idea is to always insert a new item into h1(k).

This might “kick out” another item, which we then attempt to re-insert
into its alternate position.

Insertion might not be possible if there is a loop.

In this case, we have to rehash with a larger M.

The big advantage is that an element with key k
can only be in T [h1(k)] or T[h2(k)]. (WORST CASE O(1) LOOKUP!!)

cuckoo-insert(T,x)
T : hash table, x: new item to insert
1.  y ← x, i ← h1(x.key)
2.  do at most n times:
3.      swap(y , T [i])
4.      if y is “empty” then return “success”
5.      if i = h1(y.key) then i ← h2 (y.key)
6.      else i ← h 1 (y .key )
7.  return “failure”

 By combining multiple hash functions with multiple cells per bucket, very high space utilization can be achieved.

>>>>>>>>>>>>>>> HOPSCOTCH HASHING
(TODO FINISH THIS: https://en.wikipedia.org/wiki/Hash_table#Separate_chaining_with_list_head_cells)

>>>>>>>>>>>>>>>> ROBIN HOOD HASHING

>>>>>>>>>>>>>>>> TWO CHOICE HASHING

2-choice hashing employs two different hash functions, h1(x) and h2(x), 

for the hash table. Both hash functions are used to compute two table locations. 
When an object is inserted in the table, it is placed in the table location that 
contains fewer objects (with the default being the h1(x) table location if there 
is equality in bucket size). 2-choice hashing employs the principle of 
the power of two choices.[25]


>>>>>>>>>>>>>>>> HASH TABLE CACHE PERFORMANCE EXPLAINED;

So my questions are:
What causes chaining to have a bad cache performance?

Where is the cache being used?

Why would open addressing provide better cache performance as 

I cannot see how the cache comes into this?

Also what considerations what you take into account when deciding 
between chaining and linear probed open addressing 
and quadratic probed open addressing?

Sorry, due to quite wide questions, the answers also will be 
quite generic with some links for more detailed information.

It is better to start with the question:

Where is the cache being used?

On modern CPUs, cache is used everywhere: to read program instructions 
and read/write data in memory. On most CPUs cache is transparent, i.e. 
there is no need to explicitly manage the cache.

Cache is much faster than the main memory (DRAM). Just to give 
you a perspective, accessing data in Level 1 cache is ~4 CPU cycles, 
while accessing the DRAM on the same CPU is ~200 CPU cycles, 
i.e. 50 times faster.

Cache operate on small blocks called cache lines, 
which are usually 64 bytes long.

More info: https://en.wikipedia.org/wiki/CPU_cache

What causes chaining to have a bad cache performance?

Basically, chaining is not cache friendly. It is not only 
about this case in the hash tables, 
same issue with "classical" lists.

Hash keys (or list nodes) are far away from each other, 
so each key access generates a "cache miss", i.e. 
slow DRAM access. So checking 10 keys in a chain takes 
10 DRAM accesses, i.e. 200 x 10 = 2000 cycles for our generic CPU.

The address of the next key is not known until a next pointer is 
read in the current key, so there is not much room for an optimization...

Why would open addressing provide better cache performance 
as I cannot see how the cache comes into this?

Linear probing is cache friendly. Keys are "clustered" together, 
so once we accessed the first key (slow DRAM access), most probably the 
next key will be already in cache, since the cache line is 64 bytes. 
So accessing the same 10 keys with open addressing takes 1 DRAM access 
and 9 cache accesses, i.e. 200 x 1 + 9 x 4 = 236 cycles for our generic CPU. 
It is much faster than 2000 cycles for chained keys.

Also, since we access the memory in predictable manner, there 
is a room for optimizations like cache prefetching: 
https://en.wikipedia.org/wiki/Cache_prefetching

Also what considerations what you take into account when deciding 
between chaining and linear probed open addressing and 
quadratic probed open addressing?

Chaining or linear probing is not a good sign anyway. 
So the first thing I would consider is to make sure the probability 
of collisions is at minimum by using a good hash function and reasonable hash size.

The second thing I would consider is a ready to use solution. 
Sure, there are still might be some rare 
cases when you need your own implementation...

Not sure about the language, but here is blazingly fast hash 
table implementation with BSD license: http://dpdk.org/browse/dpdk/tree/lib/librte_hash/rte_cuckoo_hash.h

So, if you still need your own hash table implementation and 
you do care about performance, the next quite easy thing to 
implement would be to use cache aligned buckets instead 
of plain hash elements. It will waste few bytes per each element 
(i.e. each hash table element will be 64 bytes long), but in case 
of a collision there will be some fast storage for at least few keys. 
The code to manage those buckets will be also a bit more complicated, 
so it is a thing to consider if it is worth for you to bother...


########################%#################################
HASHMAPS:  LOAD FACTOR AND REHASHING
How hashing works:

For insertion of a key(K) – value(V) pair into a hash map, 2 steps are required:

K is converted into a small integer (called its hash code) using a hash function.

The hash code is used to find an index (hashCode % arrSize) and the entire linked list 
at that index(Separate chaining) is first searched for the presence of the K already.

If found, it’s value is updated and if not, the K-V pair is stored as a new node in the list.
Complexity and Load Factor

For the first step, time taken depends on the K and the hash function.
For example, if the key is a string “abcd”, then it’s hash function may depend 
on the length of the string. But for very large values of n, the number of entries 
into the map, length of the keys is almost negligible in comparison to n so hash 
computation can be considered to take place in constant time, i.e, O(1).

For the second step, traversal of the list of K-V pairs present at that index needs to be done. 
For this, the worst case may be that all the n entries are at the same index. 
So, time complexity would be O(n). But, enough research has been done to make 
hash functions uniformly distribute the keys in the array so this almost never happens.

So, on an average, if there are n entries and b is the size of the array 
there would be n/b entries on each index. This value n/b is called the 
load factor that represents the load that is there on our map.

This Load Factor needs to be kept low, so that number of entries at 
one index is less and so is the complexity almost constant, i.e., O(1).

Rehashing:

As the name suggests, rehashing means hashing again. 
Basically, when the load factor increases to more than its pre-defined value 
(default value of load factor is 0.75), the complexity increases. So to overcome this, 
the size of the array is increased (doubled) and all the values are hashed again and 
stored in the new double sized array to maintain a low load factor and low complexity.

Why rehashing?

Rehashing is done because whenever key value pairs are inserted into the map, 
the load factor increases, which implies that the time complexity also increases 
as explained above. This might not give the required time complexity of O(1).

Hence, rehash must be done, increasing the size of the bucketArray so 
as to reduce the load factor and the time complexity.

How Rehashing is done?

Rehashing can be done as follows:

For each addition of a new entry to the map, check the load factor.
If it’s greater than its pre-defined value (or default value of 0.75 if not given), then Rehash.
For Rehash, make a new array of double the previous size and make it the new bucketarray.
Then traverse to each element in the old bucketArray and call the insert() 
for each so as to insert it into the new larger bucket array.

########################### HASH SET ################### (OPEN ADDRESSING SOLUTION)

This implementation was based on some data structures concepts 
and what I've read about Python's dict implementation.

The underlying structure that my hash set uses is a list. The list simply contains 
a key at the index the key is hashed to, unless there are multiple keys 
at the same index (ie a collision). More on that later...

My hashset is designed to use a limited amount of memory, but expands itself by a factor of 2 
when the load factor (size divided by array spots) exceeds 2/3. 
This will allow for O(1) operations on average. The downside to this is that every time the 
hashset doubles, it has to create a new list and rehash every element which takes O(n) each time. 
The average time will be O(1) for each add call as the cost of rehashing the set is amortized over the calls.

Open addressing and chaining each have their own advantages and disadvantages. I'm not 
going to explain the nuances of the methods, but know that chaining hash sets/tables 
have the head of a linked list of keys in each array spot, 
while open addressing just moves the key to an empty index.

I used open addressing for this problem. If we add an element that is hashed to the same 
index as another key, then we apply a secondary operation on the index until we 
find an empty spot (or a previously removed spot). This is called double hashing.
The formula I used is similar to that of the Python dict implementation, 
but without the perturb part. This is better than linear probing since the 
hash function (which just mods the key by the capacity of the list) is likely 
to fill contiguous array spots and double hashing makes the probability 
of finding an empty spot more uniform in those cases than linear probing.

For removing, the removed element is replaced by a tombstone so that 
the contains function won't get messed up when the path to an existing 
element has an empty spot in it, causing the contains function to return 
false. The add function will know that 
tombstones are able to be replaced by new elements.

class MyHashSet(object):

    def __init__(self):
        """
        Initialize your data structure here.
        """
        self.capacity = 8
        self.size = 0
        self.s = [None]*8
        self.lf = float(2)/3
        
    def myhash(self, key): # can be modified to hash other hashable objects like built in python hash function
        return key%self.capacity
        

    def add(self, key):
        """
        :type key: int
        :rtype: void
        """
        if float(self.size)/self.capacity >= self.lf:
            self.capacity <<= 1
            ns = [None]*self.capacity
            for i in range(self.capacity >> 1):
                if self.s[i] and self.s[i] != "==TOMBSTONE==":
                    n = self.myhash(self.s[i])
                    while ns[n] is not None:
                        n = (5*n+1)%self.capacity
                    ns[n] = self.s[i]
            self.s = ns
        h = self.myhash(key)
        while self.s[h] is not None:
            if self.s[h] == key:
                return
            h = (5*h + 1) % self.capacity
            if self.s[h] == "==TOMBSTONE==":
                break
        self.s[h] = key
        self.size += 1
        
        

    def remove(self, key):
        """
        :type key: int
        :rtype: void
        """
        h = self.myhash(key)
        while self.s[h]:
            if self.s[h] == key:
                self.s[h] = "==TOMBSTONE=="
                self.size -= 1
                return
            h = (5*h+1)%self.capacity
        
    def contains(self, key):
        """
        Returns true if this set contains the specified element
        :type key: int
        :rtype: bool
        """
        h = self.myhash(key)
        while self.s[h] is not None:
            if self.s[h] == key:
                return True
            h = (5*h + 1)%self.capacity
        return False


#################################### HASH MAP ################################### (OPEN CHAINING)
### THIS SOLUTION SHOULD USE LOAD FACTORS! => LIKE THE SOLUTION ABOVE.


# using just arrays, direct access table
# using linked list for chaining

class ListNode:
    def __init__(self, key, val):
        self.pair = (key, val)
        self.next = None

class MyHashMap:

    def __init__(self):
        """
        Initialize your data structure here.
        """
        self.m = 1000;
        self.h = [None]*self.m
        

    def put(self, key, value):
        """
        value will always be non-negative.
        :type key: int
        :type value: int
        :rtype: void
        """
        index = key % self.m
        if self.h[index] == None:
            self.h[index] = ListNode(key, value)
        else:
            cur = self.h[index]
            while True:
                if cur.pair[0] == key:
                    cur.pair = (key, value) #update
                    return
                if cur.next == None: break
                cur = cur.next
            cur.next = ListNode(key, value)
        

    def get(self, key):
        """
        Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key
        :type key: int
        :rtype: int
        """
        index = key % self.m
        cur = self.h[index]
        while cur:
            if cur.pair[0] == key:
                return cur.pair[1]
            else:
                cur = cur.next
        return -1
            
    def remove(self, key):
        """
        Removes the mapping of the specified value key if this map contains a mapping for the key
        :type key: int
        :rtype: void
        """
        index = key % self.m
        cur = prev = self.h[index]
        if not cur: return
        if cur.pair[0] == key:
            self.h[index] = cur.next
        else:
            cur = cur.next
            while cur:
                if cur.pair[0] == key:
                    prev.next = cur.next
                    break
                else:
                    cur, prev = cur.next, prev.next
                


# Your MyHashMap object will be instantiated and called as such:
# obj = MyHashMap()
# obj.put(key,value)
# param_2 = obj.get(key)
# obj.remove(key)


###########################################################################################################################
################################# SKIP LIST


###########################################################################################################################
################################# CIRCULAR DEQUE:


###########################################################################################################################
################################# CIRCULAR QUEUE