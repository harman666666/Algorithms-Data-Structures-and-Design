# PYTHON SET BINARY OPERATORS:

FIND INTERSECTION OF 2 ARRAYS:

    class Solution:
        def intersection(self, nums1: List[int], nums2: List[int]) -> List[int]:
            
            a = set(nums1)
            b = set(nums2)
            return a & b
        
OTHER COOL BINARY OPERATORS: <=, >=, |, &, -, ^

s <= t
test whether every element in s is in t

s >= t
test whether every element in t is in s

	
s | t
new set with elements from both s and t

	
s & t
new set with elements common to s and t

s - t
new set with elements in s but not in t

	
s ^ t
new set with elements in either s or t but not both


s.copy() new set with a shallow copy of s

s |= t
return set s with elements added from t

s &= t
return set s keeping only elements also found in t

	
s -= t
return set s after removing elements found in t


s ^= t
return set s with elements from s or t but not both

s.add(x) add element x to set s
s.remove(x) remove x from set s; raises KeyError if not present
s.discard(x) removes x from set s if present
s.pop() remove and return an arbitrary element from s; raises KeyError if empty
s.clear() remove all elements from set s


Note, the non-operator versions of update(), intersection_update(), difference_update(), 
and symmetric_difference_update() will accept any iterable as an argument.


##################################################################################
COMPREHENSIONS: 


# List comp also works as a nested loop.

[[a, b] for a in range(0, 3) for b in range(0, 5)]
Out[19]:
[[0, 0],
 [0, 1],
 [0, 2],
 [0, 3],
 [0, 4],
 [1, 0],
 [1, 1],
 [1, 2],
 [1, 3],
 [1, 4],
 [2, 0],
 [2, 1],
 [2, 2],
 [2, 3],
 [2, 4]]
Set comprehension

Return to table of contents

In [20]:
# Set comprehension is same format as list comprehension but uses curly brackets.

set_1_with_comp = {x for x in range(1, 11)}
set_2_with_comp = {x for x in range(11, 21)}
In [21]:
print(set_1_with_comp)
print(set_2_with_comp)
{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
{11, 12, 13, 14, 15, 16, 17, 18, 19, 20}
Dict comprehension

Return to table of contents

Examples from: http://cmdlinetips.com/2018/01/5-examples-using-dict-comprehension/ (More samples there as well)

In [22]:
# dict comprehension to create dict with numbers as values
{str(i):i for i in [1,2,3,4,5]}
Out[22]:
{'1': 1, '2': 2, '3': 3, '4': 4, '5': 5}
In [23]:
# create list of fruits
fruits = ["apple", "mango", "banana", "cherry"]

# dict comprehension to create dict with fruit name as keys
{f:len(f) for f in fruits}
Out[23]:
{'apple': 5, 'mango': 5, 'banana': 6, 'cherry': 6}


##################################################################################

itertools - PERMS AND COMBS

COMBINATIOS: 
    from itertools import combinations

    foo = [1, 2, 3, 4]

    for i in xrange(len(foo)):
        for j in xrange(i + 1, len(foo)):
            print foo[i], foo[j]

    for c in  combinations(foo, 2):
        print c

##################################################################################
itertools - ITERATORS 


The itertools module includes a set of functions for working with sequence data sets. Iterator-based code offers better memory consumption characteristics than code that uses lists. Since data is not produced from the iterator until it is needed, all data does not need to be stored in memory at the same time.

The count() function returns an iterator that produces consecutive integers, indefi- nitely. The first number can be passed as an argument (the default is zero).

from itertools import *

for i in izip(count(1), ['a', 'b', 'c']):
    print(i),

# (1, 'a') (2, 'b') (3, 'c') 
The cycle() function returns an iterator that indefinitely repeats the contents of the arguments it is given. Since it has to remember the entire contents of the input iterator, it may consume quite a bit of memory if the iterator is long.

from itertools import *

for i, item in izip(xrange(7), cycle(['a', 'b', 'c'])):
    print('%s. %s;' % (i, item)),
The repeat() function returns an iterator that produces the same value each time it is accessed.

from itertools import *

for i in repeat('a', 5):
    print(i),

# a a a a a
The chain() function takes several iterators as arguments and returns a single iterator that produces the contents of all of them as though they came from a single iterator.

from itertools import *

for i in chain([5, 6, 7], [14, 15, 16]):
    print(i),
# 5 6 7 14 15 16
izip() returns an iterator that combines the elements of several iterators into tuples.

from itertools import *

for i in izip([5, 6, 7], [14, 15, 16]):
    print(i),
# (5, 14) (6, 15) (7, 16)
The tee() function returns several independent iterators (defaults to 2) based on a single original input. The iterators returned by tee() can be used to feed the same set of data into multiple algorithms to be processed in parallel.

from itertools import *

r = islice(count(), 5)
i1, i2 = tee(r)

print('i1: %s' % list(i1))
print('i2: %s' % list(i2))

# i1: [0, 1, 2, 3, 4]
# i2: [0, 1, 2, 3, 4]
The imap() function returns an iterator that calls a function on the values in the input iterators and returns the results. It works like the built-in map() , except that it stops when any input iterator is exhausted (instead of inserting None values to completely consume all inputs).

from itertools import *

print('Doubles:'),
for i in imap(lambda x:2*x, xrange(5)):
    print(i),

# Doubles: 0 2 4 6 8
The dropwhile() function returns an iterator that produces elements of the input iterator after a condition becomes False for the first time. dropwhile() does not filter every item of the input; after the condition is false the first time, all remaining items in the input are returned.

from itertools import *

def should_drop(x):
    print 'Testing:', x
    return (x < 1)

for i in dropwhile(should_drop, [ -1, 0, 1, 2, -2 ]):
    print 'Yielding:', i

# Testing: -1
# Testing: 0
# Testing: 1
# Yielding: 1
# Yielding: 2
# Yielding: -2    
The opposite of dropwhile() is takewhile(). It returns an iterator that returns items from the input iterator, as long as the test function returns True.

ifilter() returns an iterator that works like the built-in filter() does for lists, including only items for which the test function returns True.

from itertools import *

def check_item(x):
    return (x < 1)

for i in ifilter(check_item, [ -1, 0, 1, 2, -2 ]):
    print('Yielding: %s' % i)

# Yielding: -1
# Yielding: 0
# Yielding: -2
ifilter() is different from dropwhile() in that every item is tested before it is returned.

ifilterfalse() returns an iterator that includes only items where the test func tion returns False.

The groupby() function returns an iterator that produces sets of values organized by a common key. It's useful for splitting up the results of large data source. It takes an iterable and a key function. The key function is used to group items with consecutively similar key values together.

from itertools import *
from operator import itemgetter

movies_by_years = [('2013', ['Lone Survivor', 'About Time']),
                   ('2011', ['Intouchables',]),
                   ('2010', ['The Next Three Days']),
                  ]

for year, items in groupby(movies_by_years, itemgetter(0)):
    print(year)
    movies = list(items)[0][1]
    for movie in movies:
        print('\t%s' % movie)

# 2013
#   Lone Survivor
#   About Time
# 2011
#   Intouchables
# 2010
#   The Next Three Days
Another example:

for key, igroup in groupby(xrange(12), lambda x: x // 5):
    print key, list(igroup)

# 0 [0, 1, 2, 3, 4]
# 1 [5, 6, 7, 8, 9]
# 2 [10, 11]
The compress() makes an iterator that filters elements from data returning only those that have a corresponding element in selectors that evaluates to True. Stops when either the data or selectors iterables has been exhausted.

from itertools import *

letters = compress('ABCDEF', [1,0,1,0,1,1])

for letter in letters:
    print(letter),

# A C E F







##################################################################################

SHORT CIRCUITING TO GET VALUES TRICK:

For any one else having the same question:
simple or statements:

The statement x or y returns x if x evaluates to True, otherwise it 
returns y (even if both are False), let us look at a few examples:

1 or 2 returns 1, because 1 evaluates to True
0 or 1 returns 1
0 or '' returns ''

simple and statements:
The statement x and y returns x if x evaluates to False, otherwise it returns y (even if both are True), 

let us look at a few examples:
1 and 2 returns 2
1 and 0 returns 0
0 and 2 returns 0
So we can see that intuitively the above statements are evaluated in a "short circuit" manner, 
i.e. the statement is evaluated from left to right and whichever variable determines 
the value of the boolean expression first is returned.

Now let us have a look at some complicated examples with the above intuition:
Continuous or statements
The variable which first evaluates to True is returned else the last variable is returned.
Examples:
1 or 2 or 3 or 4 returns 1
'' or {} or [] or 4 returns 4
'' or {} or [] returns []
Continuous and statements
The variable which first evaluates to False is returned else the last variable is returned
Examples:
1 and 2 and 3 returns 3
1 and '' and 4 returns ''
0 and '' and 4 returns 0

More complex statements:
What if compound statements mixed with and and or are used?
In such cases we have to remember that and has a higher precedence than 
or and the order of evaluation of the expression is always from left to right, 
and lastly remember the 'short circuit' intuition.

Let us have a look at some statements:
1 and 2 or 4 returns 2, why?
1 and 2 or 4 is evaluated as (1 and 2) or (4), (1 and 2) evaluates to True, 
so the or statement returns 1 and 2 which further returns 2

Infact the ternary condition x if condition else y can be rewritten as
condition and x or y ! (prove it to yourself) , although it is not recommended for readability purposes.



################################################################################33
PRIORITY QUEUE:
HEAPQ IS A MIN HEAP BY DEFAULT.

how to make it a max heap?

The easiest and ideal solution
Multiply the values by -1
There you go. All the highest numbers are now the lowest and vice versa.
Just remember that when you pop an element to multiply it with -1 
in order to get the original value again.


HEAPQ API:

These two make it possible to view the heap as a regular Python 
list without surprises: heap[0] is the smallest item, 
and heap.sort() maintains the heap invariant!

To create a heap, use a list initialized to [], or you can 
transform a populated list into a heap via function heapify().

heapq.heappush(heap, item)
Push the value item onto the heap, maintaining the heap invariant.

heapq.heappop(heap)
Pop and return the smallest item from the heap, maintaining the heap invariant. 
If the heap is empty, IndexError is raised. To access the smallest 
item without popping it, use heap[0].

heapq.heappushpop(heap, item)
Push item on the heap, then pop and return the smallest item from the heap. 
The combined action runs more efficiently than heappush() 
followed by a separate call to heappop().

heapq.heapify(x)
Transform list x into a heap, in-place, in linear time.

heapq.heapreplace(heap, item)
Pop and return the smallest item from the heap, and also push the new item. 
The heap size doesn’t change. If the heap is empty, IndexError is raised.

This one step operation is more efficient than a heappop() followed by heappush() and
can be more appropriate when using a fixed-size heap. The pop/push combination 
always returns an element from the heap and replaces it with item.

The value returned may be larger than the item added. If that isn’t desired, 
consider using heappushpop() instead. Its push/pop combination returns the 
smaller of the two values, leaving the larger value on the heap.

The module also offers three general purpose functions based on heaps.

heapq.merge(*iterables)
Merge multiple sorted inputs into a single 
sorted output (for example, merge timestamped 
entries from multiple log files). Returns an iterator over the sorted values.

Similar to sorted(itertools.chain(*iterables)) but returns an iterable, 
does not pull the data into memory all at once, and assumes 
that each of the input streams is already sorted (smallest to largest).

New in version 2.6.

heapq.nlargest(n, iterable[, key])
Return a list with the n largest elements from the 
dataset defined by iterable. key, if provided, 
specifies a function of one argument that is used 
to extract a comparison key from each element in the 
iterable: key=str.lower Equivalent to: sorted(iterable, key=key, reverse=True)[:n]

New in version 2.4.

Changed in version 2.5: Added the optional key argument.

heapq.nsmallest(n, iterable[, key])
Return a list with the n smallest elements from 
the dataset defined by iterable. key, if provided, 
specifies a function of one argument that is used to 
extract a comparison key from each element in the iterable: 
key=str.lower Equivalent to: sorted(iterable, key=key)[:n]

New in version 2.4.

Changed in version 2.5: Added the optional key argument.

The latter two functions perform best for smaller 
values of n. For larger values, it is more efficient 
to use the sorted() function. Also, when n==1, it is 
more efficient to use the built-in min() and max() functions. 
If repeated usage of these functions is required, consider 
turning the iterable into an actual heap.
##########################################################################################

USING HEAPQs operations:


To implement "decrease-key" effectively, you'd need to access the functionality "decrement 
this element AND swap this element with a child until heap condition is restore". In heapq.py, 
that's called _siftdown (and similarly _siftup for INcrementing). So the good news is that the 
functions are there... the bad news is that their names start with an underscore, indicating they're 
considered "internal implementation details" and should not be accessed directly by application code 
(the next release of the standard library might change things around and break code using such "internals").

It's up to you to decide whether you want to ignore the warning leading-_, use O(N) heapify instead of 
O(log N) sifting, or reimplement some or all of heapq's functionality to make the sifting primitives 
"exposed as public parts of the interface". Since heapq's data structure is documented and public (just a list), 
I think the best choice is probably a partial-reimplementation -- copy the sifting functions 
from heapq.py into your application code, essentially.

2
The link to heapq.py seems to be stale. For convenience here is another link to the python implementation: hg.python.org/cpython/file/2.7/Lib/heapq.py – Jordan Sep 17 '13 at 12:02
2
do you mean "swap this element with its parent until heap condition is restored"? (i assumed if there were elements, [2, 3, 5], then 2 would be the parent, and 3 and 5 would be its two children) – tscizzle Oct 22 '16 at 21:16 
It should be noted that even if you can implement "decrease-key" or more generically "update-key", that functionality presumes that you have a way to track indices on the heap, so that you can pinpoint which item you want to operate on (otherwise you might have to search for it in linear time). The first obvious solution would be to augment your heap structure with a key-to-index hashmap. From then, heap changing operations (such as _siftup and _siftdown) should trigger an update of the map. – Michael Ekoka Sep 8 '19 at 8:13 

+
−
# 'heap' is a heap at all indices >= startpos, except possibly for pos.  pos
# is the index of a leaf with a possibly out-of-order value.  Restore the
# heap invariant.
def _siftdown(heap, startpos, pos):
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if cmp_lt(newitem, parent):
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem

# The child indices of heap index pos are already heaps, and we want to make
# a heap at index pos too.  We do this by bubbling the smaller child of
# pos up (and so on with that child's children, etc) until hitting a leaf,
# then using _siftdown to move the oddball originally at index pos into place.
#
# We *could* break out of the loop as soon as we find a pos where newitem <=
# both its children, but turns out that's not a good idea, and despite that
# many books write the algorithm that way.  During a heap pop, the last array
# element is sifted in, and that tends to be large, so that comparing it
# against values starting from the root usually doesn't pay (= usually doesn't
# get us out of the loop early).  See Knuth, Volume 3, where this is
# explained and quantified in an exercise.
#
# Cutting the # of comparisons is important, since these routines have no
# way to extract "the priority" from an array element, so that intelligence
# is likely to be hiding in custom __cmp__ methods, or in array elements
# storing (priority, record) tuples.  Comparisons are thus potentially
# expensive.
#
# On random arrays of length 1000, making this change cut the number of
# comparisons made by heapify() a little, and those made by exhaustive
# heappop() a lot, in accord with theory.  Here are typical results from 3
# runs (3 just to demonstrate how small the variance is):
#
# Compares needed by heapify     Compares needed by 1000 heappops
# --------------------------     --------------------------------
# 1837 cut to 1663               14996 cut to 8680
# 1855 cut to 1659               14966 cut to 8678
# 1847 cut to 1660               15024 cut to 8703
#
# Building the heap by using heappush() 1000 times instead required
# 2198, 2148, and 2219 compares:  heapify() is more efficient, when
# you can use it.
#
# The total compares needed by list.sort() on the same lists were 8627,
# 8627, and 8632 (this should be compared to the sum of heapify() and
# heappop() compares):  list.sort() is (unsurprisingly!) more efficient
# for sorting.

def _siftup(heap, pos):
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the smaller child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of smaller child.
        rightpos = childpos + 1
        if rightpos < endpos and not cmp_lt(heap[childpos], heap[rightpos]):
            childpos = rightpos
        # Move the smaller child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown(heap, startpos, pos)

def _siftdown_max(heap, startpos, pos):
    'Maxheap variant of _siftdown'
    newitem = heap[pos]
    # Follow the path to the root, moving parents down until finding a place
    # newitem fits.
    while pos > startpos:
        parentpos = (pos - 1) >> 1
        parent = heap[parentpos]
        if cmp_lt(parent, newitem):
            heap[pos] = parent
            pos = parentpos
            continue
        break
    heap[pos] = newitem

def _siftup_max(heap, pos):
    'Maxheap variant of _siftup'
    endpos = len(heap)
    startpos = pos
    newitem = heap[pos]
    # Bubble up the larger child until hitting a leaf.
    childpos = 2*pos + 1    # leftmost child position
    while childpos < endpos:
        # Set childpos to index of larger child.
        rightpos = childpos + 1
        if rightpos < endpos and not cmp_lt(heap[rightpos], heap[childpos]):
            childpos = rightpos
        # Move the larger child up.
        heap[pos] = heap[childpos]
        pos = childpos
        childpos = 2*pos + 1
    # The leaf at pos is empty now.  Put newitem there, and bubble it up
    # to its final resting place (by sifting its parents down).
    heap[pos] = newitem
    _siftdown_max(heap, startpos, pos)

##############################################################################################3

ORDERED DICT

OrderedDict preserves the order in which the keys are inserted. 
A regular dict doesn’t track the insertion order, and iterating 
it gives the values in an arbitrary order. By contrast, the order the 
items are inserted is remembered by OrderedDict.

Important Points:

Key value Change: If the value of a certain key is changed, the position of the key remains unchanged in OrderedDict.

Deletion and Re-Inserting: Deleting and re-inserting the same key will push it to the back 
                           as OrderedDict however maintains the order of insertion.


Ordered Dict can be used as a stack with the help of popitem function. 
Try implementing LRU cache with Ordered Dict.

from collections import OrderedDict 
  
print("Before deleting:\n") 
od = OrderedDict() 
od['a'] = 1
od['b'] = 2
od['c'] = 3
od['d'] = 4
  
for key, value in od.items(): 
    print(key, value) 
  
print("\nAfter deleting:\n") 
od.pop('c') 
for key, value in od.items(): 
    print(key, value) 
  
print("\nAfter re-inserting:\n") 
od['c'] = 3
for key, value in od.items(): 
    print(key, value) 

##########################################################################################################

8.5. bisect — Array bisection algorithm
New in version 2.1.


This module provides support for maintaining a list in sorted order without 
having to sort the list after each insertion. For long lists of items with 
expensive comparison operations, this can be an improvement over the more common approach. 
The module is called bisect because it uses a basic bisection algorithm to do its work. 
The source code may be most useful as a working example of the algorithm 
(the boundary conditions are already right!).


The following functions are provided:

bisect.bisect_left(a, x, lo=0, hi=len(a))
Locate the insertion point for x in a to maintain sorted order. 
The parameters lo and hi may be used to specify a subset of the list 
which should be considered; by default the entire list is used. If x is
already present in a, the insertion point will be before (to the left of) 
any existing entries. The return value is suitable for use as the first 
parameter to list.insert() assuming that a is already sorted.

The returned insertion point i partitions the array a into two halves so that 
all(val < x for val in a[lo:i]) for the left side and all(val >= x for val in a[i:hi]) for the right side.

bisect.bisect_right(a, x, lo=0, hi=len(a))
bisect.bisect(a, x, lo=0, hi=len(a))

Similar to bisect_left(), but returns an insertion point which 
comes after (to the right of) any existing entries of x in a.

The returned insertion point i partitions the array a into two 
halves so that all(val <= x for val in a[lo:i]) for the left side 
and all(val > x for val in a[i:hi]) for the right side.

bisect.insort_left(a, x, lo=0, hi=len(a))
Insert x in a in sorted order. This is 
equivalent to a.insert(bisect.bisect_left(a, x, lo, hi), x) assuming that a 
is already sorted. Keep in mind that the O(log n) search 
is dominated by the slow O(n) insertion step.

bisect.insort_right(a, x, lo=0, hi=len(a))
bisect.insort(a, x, lo=0, hi=len(a))
Similar to insort_left(), but inserting x in a after any existing entries of x.

##############################################################################

FROZEN SETS, AND HASHING PERFORMANCE:

    652. Find Duplicate Subtrees
    Medium
    1261
    198

    Given a binary tree, return all duplicate subtrees. 
    For each kind of duplicate subtrees, you only need to 
    return the root node of any one of them.

    STEFANS SOLUTION: 

    First the basic version, which is O(n2) 
    time and gets accepted in about 150 ms:

    def findDuplicateSubtrees(self, root):
        def tuplify(root):
            if root:
                tuple = root.val, tuplify(root.left), tuplify(root.right)
                trees[tuple].append(root)
                return tuple
        trees = collections.defaultdict(list)
        tuplify(root)
        return [roots[0] for roots in trees.values() if roots[1:]]

    I convert the entire tree of nested TreeNodes to a tree of nested tuples. 
    Those have the advantage that they already support hashing and deep 
    comparison (for the very unlikely cases of hash collisions). So 
    then I can just use each subtree's tuple version as a key in my 
    dictionary. And equal subtrees have the same key and thus get 
    collected in the same list.

    Overall this costs only O(n) memory (where n is the number of 
    nodes in the given tree). The string serialization I've seen 
    in other posted solutions costs O(n^2) memory (and thus also at least that much time).


    So far only O(n2) time
    Unfortunately, tuples don't cache their own hash value 
    (see this for a reason). So if I use a tuple as key and thus 
    it gets asked for its hash value, it will compute it again. 
    Which entails asking its content elements for their hashes. 
    And if they're tuples, then they'll do the same and ask 
    their elements for their hashes. And so on. So asking a 
    tuple tree root for its hash traverses the entire tree. 
    Which makes the above solution only O(n^2) time, as the 
    following test demonstrates. It tests linear trees, and 
    doubling the height quadruples the run time, exactly 
    what's expected from a quadratic time algorithm.

    The code:

    from timeit import timeit
    import sys
    sys.setrecursionlimit(5000)

    def tree(height):
        if height:
            root = TreeNode(0)
            root.right = tree(height - 1)
            return root

    solution = Solution().findDuplicateSubtrees
    for e in range(5, 12):
        root = tree(2**e)
        print(timeit(lambda: solution(root), number=1000))
    
    Caching hashes
    There's an easy way to add caching, though. 
    Simply wrap each tuple in a frozenset, which 
    does cache its hash value:

    def findDuplicateSubtrees(self, root):
        def convert(root):
            if root:
                result = frozenset([(root.val, convert(root.left), convert(root.right))])
                trees[result].append(root)
                return result
        trees = collections.defaultdict(list)
        convert(root)
        return [roots[0] for roots in trees.values() if roots[1:]]
    
    Running the above test again now shows O(n) behaviour as 
    expected, doubling of size causing doubling of run time:

    There is an O(N) solution check the question file. 
